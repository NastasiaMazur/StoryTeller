{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NastasiaMazur/StoryTeller/blob/main/StoryTeller_3_books.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Story Teller - ML"
      ],
      "metadata": {
        "id": "hjE-iirdP_Rh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbs4k0MJifjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80e0a26-de24-470f-a791-ddb9bad0427c"
      },
      "source": [
        "#read-PDF imports here\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "#pre-processing imports here\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBKuc_dR-Fgt",
        "outputId": "314da3eb-ad07-4612-9b82-18f31eba3045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhwcUudNChx8"
      },
      "source": [
        "#file locations on drive\n",
        "grimm_url = '/content/drive/MyDrive/Story_Teller/FairytalesByTheBrothersGrimm.txt'\n",
        "coraline_url = '/content/drive/MyDrive/Story_Teller/Coraline.pdf'\n",
        "alice_url = '/content/drive/MyDrive/Story_Teller/AlicesAdvanturesInWonderland.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIx1pysjBnD"
      },
      "source": [
        "#load punctuation symbols\n",
        "punct = string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r7JQDi3iz14"
      },
      "source": [
        "# **Pre-processing Coraline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vand5igZEP5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3bd7db8-eb04-4e43-b369-75c40743091f"
      },
      "source": [
        "#a function to pre process Coraline by Neil Gaiman\n",
        "import PyPDF2\n",
        "def preprocess_coraline(book):\n",
        "  '''\n",
        "  param book: url od a PDF book file\n",
        "  '''\n",
        "  output = \"\"                                     # to store the text extracted from the PDF\n",
        "  data = open(book, 'rb')\n",
        "  data = PyPDF2.PdfReader(data)                   # opens the PDF file in binary mode and reads it using PyPDF2 library\n",
        "  npages = len(data.pages)                        # determines the number of pages in the PDF\n",
        "  for i in range(npages):                         # iterates through each page of the PDF, extracts the text from each page, and appends it to the output string\n",
        "    page_i = data.pages[i].extract_text()\n",
        "    output += page_i\n",
        "  output = output[1227:]                          # removes the first 1227 characters from output string: title, author, table of contents, introductory quote\n",
        "  output = output.lower()\n",
        "  for word in output:\n",
        "    for char in word:\n",
        "        if char in punct:\n",
        "            word = word.replace(char, \"\")\n",
        "  remove_punct = \"\".join([word for word in output if word not in punct])\n",
        "  processed = word_tokenize(remove_punct)\n",
        "  print('Coraline database includes {} tokens, and {} unique tokens after editing'.format(len(processed), len(set(processed))))\n",
        "  return processed\n",
        "\n",
        "coraline = preprocess_coraline(coraline_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coraline database includes 33352 tokens, and 3660 unique tokens after editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRDl2y87jh1w"
      },
      "source": [
        "## **Preprocessing Alice in Wonderland**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6xyaeRuiMnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6463cf-ec5b-4702-b323-9ca454a45170"
      },
      "source": [
        "#a function to pre process Alice's Advantures in Wonderland by Lewis Carroll\n",
        "\n",
        "def load_alice(text_file, punct, not_a_word):\n",
        "    '''\n",
        "    param text_file: url to Project Gutenberg's text file for Alice's Advantures in Wonderland by Lewis Carroll\n",
        "    param punct: a string of punctuation characters we'd like to filter\n",
        "    param not_a_word: a list of words we'd like to filter\n",
        "    '''\n",
        "    book = open(text_file, 'r')\n",
        "    book = book.read()\n",
        "    book = book[715:145060]\n",
        "    book_edit = re.sub('[+]', '', book)\n",
        "    book_edit = re.sub(r'(CHAPTER \\w+.\\s)', '', book)\n",
        "    words = word_tokenize(book_edit.lower())\n",
        "\n",
        "    word_list = []\n",
        "\n",
        "    # filtering punctuation and non-words\n",
        "    for word in words:\n",
        "        for char in word:\n",
        "            if char in punct:\n",
        "                word = word.replace(char, \"\")\n",
        "        if word not in punct and word not in not_a_word:\n",
        "            word_list.append(word)\n",
        "\n",
        "    print('Alice database includes {} tokens, and {} unique tokens after editing'.format(len(word_list), len(set(word_list))))\n",
        "    return word_list\n",
        "\n",
        "alice = load_alice(alice_url, (punct.replace('-', \"\") + '’' + '‘'), ['s', '--', 'nt', 've', 'll', 'd'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice database includes 26612 tokens, and 2596 unique tokens after editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcUOknolEWm"
      },
      "source": [
        "# **Preprocessing Grimm**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTG-MTfsEgNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e8ef78d-05fe-4a9a-99b6-07c6070c1dd9"
      },
      "source": [
        "def load_fairytales(text_file):\n",
        "    '''\n",
        "    param text_file: url to Project Gutenberg's text file for Fairytales by The Brothers Grimm\n",
        "    '''\n",
        "    book = open(text_file, encoding='cp1252')\n",
        "    book = book.read()\n",
        "    book = book[2376:519859]\n",
        "    book_edit = re.sub('[(+*)]', '', book)\n",
        "    words = word_tokenize(book_edit.lower())\n",
        "\n",
        "    # filtering punctuation inside tokens (example: didn't or wow!)\n",
        "    for word in words:\n",
        "        for char in word:\n",
        "            if char in punct:\n",
        "                word = word.replace(char, \"\")\n",
        "\n",
        "    # filtering punctuation as alone standing tokens(example: \\ or ,)\n",
        "    words = [word for word in words if word not in punct]\n",
        "\n",
        "    print('Fairytales database includes {} tokens, and {} unique tokens after editing'.format(len(words), len(set(words))))\n",
        "    return words\n",
        "\n",
        "brothers_grimm = load_fairytales(grimm_url)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairytales database includes 106324 tokens, and 5335 unique tokens after editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM8boZfBhTaw"
      },
      "source": [
        "# **Combined database including all books**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROjnov9uWSbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e13eba5-ffc9-40fb-be58-200ce908f427"
      },
      "source": [
        "data = coraline + alice + brothers_grimm\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beaten',\n",
              " '—g',\n",
              " 'k',\n",
              " 'chesterton',\n",
              " '1',\n",
              " 'coraline',\n",
              " 'discovered',\n",
              " 'the',\n",
              " 'door',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84kEdfzunnVY"
      },
      "source": [
        "# **Convert Data into Numeric Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH5YsCDlm6jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfccd04-bc4c-4f83-8f4e-37639553810e"
      },
      "source": [
        "vocab = set(data)\n",
        "vocab_size = len(data)\n",
        "\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "data = [word_to_index[word] for word in data]    # list comprehension\n",
        "\n",
        "data [:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7625, 7901, 5861, 5991, 511, 3471, 5142, 6695, 4068, 7764]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['beaten']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAeR3ZE6uiKp",
        "outputId": "6cb4b2b1-e150-4aa1-fedb-67d54d608e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7625"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_IEytwGn0jC"
      },
      "source": [
        "# **Batching Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP5qnVKWn5gW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9778dad8-5a79-4821-d6d5-fdb96c404dea"
      },
      "source": [
        "batch_size = 5 # look into first 5 words in each batch\n",
        "\n",
        "train_data = [([data[i], data[i+1],data[i+2], data[i+3], data[i+4]], data[i+5]) for i in range(vocab_size - batch_size)] #features + target word\n",
        "\n",
        "train_data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[([7625, 7901, 5861, 5991, 511], 3471),\n",
              " ([7901, 5861, 5991, 511, 3471], 5142),\n",
              " ([5861, 5991, 511, 3471, 5142], 6695),\n",
              " ([5991, 511, 3471, 5142, 6695], 4068),\n",
              " ([511, 3471, 5142, 6695, 4068], 7764),\n",
              " ([3471, 5142, 6695, 4068, 7764], 3343),\n",
              " ([5142, 6695, 4068, 7764, 3343], 6947),\n",
              " ([6695, 4068, 7764, 3343, 6947], 8067),\n",
              " ([4068, 7764, 3343, 6947, 8067], 6617),\n",
              " ([7764, 3343, 6947, 8067, 6617], 6975)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU2YCOBVn6RU"
      },
      "source": [
        "# **Defining the Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMpb7olaoASw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "embedding_dim = 5\n",
        "\n",
        "class StoryTeller(nn.Module):\n",
        "  def __init__ (self, vocab_size, embedding_dim, batch_size):\n",
        "    super(StoryTeller, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(batch_size * embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, 512)\n",
        "    self.linear3 = nn.Linear(512, vocab_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((1,-1))\n",
        "    out = F.relu(self.linear1(embeds))\n",
        "    out = F.relu(self.linear2(out))\n",
        "    out = self.linear3(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = StoryTeller(vocab_size, embedding_dim, batch_size)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAmGIK5wzNWZ",
        "outputId": "8d0695c7-bbde-46cf-d60b-1b0f81919c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StoryTeller(\n",
              "  (embeddings): Embedding(166288, 5)\n",
              "  (linear1): Linear(in_features=25, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
              "  (linear3): Linear(in_features=512, out_features=166288, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcBctDGoA_5"
      },
      "source": [
        "# **Defining Training Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj_jF78uoGqy"
      },
      "source": [
        "average_loss = []\n",
        "def train (model, train_data, epochs, word_to_index):\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Training on GPU...\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Training on CPU...\")\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  for i in range(epochs):\n",
        "    model.train()\n",
        "    steps = 0\n",
        "    print_every = 100\n",
        "    running_loss = 0\n",
        "    for feature, target in train_data:\n",
        "      feature_tensor = torch.tensor(feature, dtype=torch.long).unsqueeze(0)  # Add a batch dimension\n",
        "      feature_tensor = feature_tensor.to(device)\n",
        "      target_tensor = torch.tensor([target], dtype=torch.long)  # Reshape to remove batch dimension\n",
        "      target_tensor = target_tensor.to(device)\n",
        "      model.zero_grad()\n",
        "      log_probs = model(feature_tensor)\n",
        "      loss = criterion(log_probs, target_tensor)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      steps += 1\n",
        "      \"\"\"\n",
        "      feature_tensor = torch.tensor([feature], dtype = torch.long)\n",
        "      feature_tensor = feature_tensor.to(device)\n",
        "      #target_tensor = torch.tensor([feature], dtype = torch.long)\n",
        "      target_tensor = torch.tensor(target, dtype=torch.long)\n",
        "      target_tensor = target_tensor.to(device)\n",
        "      model.zero_grad() #set gradients to 0\n",
        "      log_probs = model(feature_tensor) #pass feature tensor into the model\n",
        "      loss = criterion(log_probs, target_tensor)\n",
        "      loss.backword()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "      step +=1\n",
        "\"\"\"\n",
        "      if steps % print_every == 0:\n",
        "        model.eval()\n",
        "        average_loss.append(running_loss/print_every)\n",
        "        print(\"Epochs: {} / {}\".format(i+1, epochs),\n",
        "              \"Training Loss: {:.3f} \".format(running_loss / print_every))\n",
        "        running_loss = 0\n",
        "      model.train()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kc2ysQGoJZU"
      },
      "source": [
        "# **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUYN20PVoMvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d9fce0-a559-4ccc-a3d0-0d075749c47f"
      },
      "source": [
        "model = StoryTeller(vocab_size, embedding_dim, batch_size)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "epochs = 10\n",
        "device = 0\n",
        "\n",
        "start_time =time.time()\n",
        "model = train(model, train_data, epochs, word_to_index)\n",
        "\n",
        "print(\"training took {} minutes\".format(round((start_time - time.time())/60), 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU...\n",
            "Epochs: 1 / 10 Training Loss: 11.977 \n",
            "Epochs: 1 / 10 Training Loss: 11.845 \n",
            "Epochs: 1 / 10 Training Loss: 11.703 \n",
            "Epochs: 1 / 10 Training Loss: 11.530 \n",
            "Epochs: 1 / 10 Training Loss: 11.040 \n",
            "Epochs: 1 / 10 Training Loss: 10.545 \n",
            "Epochs: 1 / 10 Training Loss: 9.927 \n",
            "Epochs: 1 / 10 Training Loss: 9.604 \n",
            "Epochs: 1 / 10 Training Loss: 9.263 \n",
            "Epochs: 1 / 10 Training Loss: 9.206 \n",
            "Epochs: 1 / 10 Training Loss: 8.900 \n",
            "Epochs: 1 / 10 Training Loss: 9.488 \n",
            "Epochs: 1 / 10 Training Loss: 8.173 \n",
            "Epochs: 1 / 10 Training Loss: 8.531 \n",
            "Epochs: 1 / 10 Training Loss: 7.902 \n",
            "Epochs: 1 / 10 Training Loss: 8.796 \n",
            "Epochs: 1 / 10 Training Loss: 8.381 \n",
            "Epochs: 1 / 10 Training Loss: 8.204 \n",
            "Epochs: 1 / 10 Training Loss: 7.915 \n",
            "Epochs: 1 / 10 Training Loss: 7.951 \n",
            "Epochs: 1 / 10 Training Loss: 8.519 \n",
            "Epochs: 1 / 10 Training Loss: 8.268 \n",
            "Epochs: 1 / 10 Training Loss: 8.306 \n",
            "Epochs: 1 / 10 Training Loss: 7.807 \n",
            "Epochs: 1 / 10 Training Loss: 7.909 \n",
            "Epochs: 1 / 10 Training Loss: 8.002 \n",
            "Epochs: 1 / 10 Training Loss: 8.126 \n",
            "Epochs: 1 / 10 Training Loss: 7.537 \n",
            "Epochs: 1 / 10 Training Loss: 6.830 \n",
            "Epochs: 1 / 10 Training Loss: 7.246 \n",
            "Epochs: 1 / 10 Training Loss: 8.067 \n",
            "Epochs: 1 / 10 Training Loss: 8.774 \n",
            "Epochs: 1 / 10 Training Loss: 7.481 \n",
            "Epochs: 1 / 10 Training Loss: 7.408 \n",
            "Epochs: 1 / 10 Training Loss: 7.430 \n",
            "Epochs: 1 / 10 Training Loss: 7.079 \n",
            "Epochs: 1 / 10 Training Loss: 7.638 \n",
            "Epochs: 1 / 10 Training Loss: 8.575 \n",
            "Epochs: 1 / 10 Training Loss: 7.347 \n",
            "Epochs: 1 / 10 Training Loss: 7.828 \n",
            "Epochs: 1 / 10 Training Loss: 7.478 \n",
            "Epochs: 1 / 10 Training Loss: 7.851 \n",
            "Epochs: 1 / 10 Training Loss: 7.222 \n",
            "Epochs: 1 / 10 Training Loss: 7.804 \n",
            "Epochs: 1 / 10 Training Loss: 7.552 \n",
            "Epochs: 1 / 10 Training Loss: 7.218 \n",
            "Epochs: 1 / 10 Training Loss: 7.502 \n",
            "Epochs: 1 / 10 Training Loss: 6.881 \n",
            "Epochs: 1 / 10 Training Loss: 7.033 \n",
            "Epochs: 1 / 10 Training Loss: 7.512 \n",
            "Epochs: 1 / 10 Training Loss: 7.587 \n",
            "Epochs: 1 / 10 Training Loss: 7.291 \n",
            "Epochs: 1 / 10 Training Loss: 7.853 \n",
            "Epochs: 1 / 10 Training Loss: 7.475 \n",
            "Epochs: 1 / 10 Training Loss: 7.850 \n",
            "Epochs: 1 / 10 Training Loss: 7.663 \n",
            "Epochs: 1 / 10 Training Loss: 7.525 \n",
            "Epochs: 1 / 10 Training Loss: 6.718 \n",
            "Epochs: 1 / 10 Training Loss: 7.437 \n",
            "Epochs: 1 / 10 Training Loss: 7.321 \n",
            "Epochs: 1 / 10 Training Loss: 6.522 \n",
            "Epochs: 1 / 10 Training Loss: 7.173 \n",
            "Epochs: 1 / 10 Training Loss: 7.787 \n",
            "Epochs: 1 / 10 Training Loss: 7.344 \n",
            "Epochs: 1 / 10 Training Loss: 6.908 \n",
            "Epochs: 1 / 10 Training Loss: 6.925 \n",
            "Epochs: 1 / 10 Training Loss: 7.173 \n",
            "Epochs: 1 / 10 Training Loss: 7.097 \n",
            "Epochs: 1 / 10 Training Loss: 7.790 \n",
            "Epochs: 1 / 10 Training Loss: 7.202 \n",
            "Epochs: 1 / 10 Training Loss: 7.341 \n",
            "Epochs: 1 / 10 Training Loss: 7.753 \n",
            "Epochs: 1 / 10 Training Loss: 7.721 \n",
            "Epochs: 1 / 10 Training Loss: 7.019 \n",
            "Epochs: 1 / 10 Training Loss: 6.954 \n",
            "Epochs: 1 / 10 Training Loss: 6.272 \n",
            "Epochs: 1 / 10 Training Loss: 7.274 \n",
            "Epochs: 1 / 10 Training Loss: 7.305 \n",
            "Epochs: 1 / 10 Training Loss: 6.632 \n",
            "Epochs: 1 / 10 Training Loss: 6.319 \n",
            "Epochs: 1 / 10 Training Loss: 6.311 \n",
            "Epochs: 1 / 10 Training Loss: 6.608 \n",
            "Epochs: 1 / 10 Training Loss: 6.866 \n",
            "Epochs: 1 / 10 Training Loss: 6.405 \n",
            "Epochs: 1 / 10 Training Loss: 6.726 \n",
            "Epochs: 1 / 10 Training Loss: 7.431 \n",
            "Epochs: 1 / 10 Training Loss: 7.026 \n",
            "Epochs: 1 / 10 Training Loss: 7.666 \n",
            "Epochs: 1 / 10 Training Loss: 7.269 \n",
            "Epochs: 1 / 10 Training Loss: 8.191 \n",
            "Epochs: 1 / 10 Training Loss: 8.005 \n",
            "Epochs: 1 / 10 Training Loss: 7.377 \n",
            "Epochs: 1 / 10 Training Loss: 7.086 \n",
            "Epochs: 1 / 10 Training Loss: 6.869 \n",
            "Epochs: 1 / 10 Training Loss: 6.776 \n",
            "Epochs: 1 / 10 Training Loss: 6.204 \n",
            "Epochs: 1 / 10 Training Loss: 6.690 \n",
            "Epochs: 1 / 10 Training Loss: 7.248 \n",
            "Epochs: 1 / 10 Training Loss: 7.530 \n",
            "Epochs: 1 / 10 Training Loss: 7.567 \n",
            "Epochs: 1 / 10 Training Loss: 6.620 \n",
            "Epochs: 1 / 10 Training Loss: 6.832 \n",
            "Epochs: 1 / 10 Training Loss: 6.328 \n",
            "Epochs: 1 / 10 Training Loss: 7.420 \n",
            "Epochs: 1 / 10 Training Loss: 7.029 \n",
            "Epochs: 1 / 10 Training Loss: 6.616 \n",
            "Epochs: 1 / 10 Training Loss: 7.327 \n",
            "Epochs: 1 / 10 Training Loss: 6.548 \n",
            "Epochs: 1 / 10 Training Loss: 7.174 \n",
            "Epochs: 1 / 10 Training Loss: 6.590 \n",
            "Epochs: 1 / 10 Training Loss: 7.011 \n",
            "Epochs: 1 / 10 Training Loss: 7.591 \n",
            "Epochs: 1 / 10 Training Loss: 7.172 \n",
            "Epochs: 1 / 10 Training Loss: 6.805 \n",
            "Epochs: 1 / 10 Training Loss: 6.879 \n",
            "Epochs: 1 / 10 Training Loss: 6.857 \n",
            "Epochs: 1 / 10 Training Loss: 7.169 \n",
            "Epochs: 1 / 10 Training Loss: 6.699 \n",
            "Epochs: 1 / 10 Training Loss: 7.067 \n",
            "Epochs: 1 / 10 Training Loss: 6.740 \n",
            "Epochs: 1 / 10 Training Loss: 6.621 \n",
            "Epochs: 1 / 10 Training Loss: 7.024 \n",
            "Epochs: 1 / 10 Training Loss: 5.824 \n",
            "Epochs: 1 / 10 Training Loss: 7.062 \n",
            "Epochs: 1 / 10 Training Loss: 7.060 \n",
            "Epochs: 1 / 10 Training Loss: 7.110 \n",
            "Epochs: 1 / 10 Training Loss: 6.575 \n",
            "Epochs: 1 / 10 Training Loss: 6.832 \n",
            "Epochs: 1 / 10 Training Loss: 7.174 \n",
            "Epochs: 1 / 10 Training Loss: 6.241 \n",
            "Epochs: 1 / 10 Training Loss: 6.716 \n",
            "Epochs: 1 / 10 Training Loss: 7.772 \n",
            "Epochs: 1 / 10 Training Loss: 5.839 \n",
            "Epochs: 1 / 10 Training Loss: 6.720 \n",
            "Epochs: 1 / 10 Training Loss: 7.714 \n",
            "Epochs: 1 / 10 Training Loss: 6.423 \n",
            "Epochs: 1 / 10 Training Loss: 6.344 \n",
            "Epochs: 1 / 10 Training Loss: 7.175 \n",
            "Epochs: 1 / 10 Training Loss: 7.008 \n",
            "Epochs: 1 / 10 Training Loss: 7.277 \n",
            "Epochs: 1 / 10 Training Loss: 6.635 \n",
            "Epochs: 1 / 10 Training Loss: 6.866 \n",
            "Epochs: 1 / 10 Training Loss: 6.512 \n",
            "Epochs: 1 / 10 Training Loss: 6.647 \n",
            "Epochs: 1 / 10 Training Loss: 6.676 \n",
            "Epochs: 1 / 10 Training Loss: 7.203 \n",
            "Epochs: 1 / 10 Training Loss: 6.715 \n",
            "Epochs: 1 / 10 Training Loss: 6.627 \n",
            "Epochs: 1 / 10 Training Loss: 7.234 \n",
            "Epochs: 1 / 10 Training Loss: 6.637 \n",
            "Epochs: 1 / 10 Training Loss: 6.941 \n",
            "Epochs: 1 / 10 Training Loss: 6.478 \n",
            "Epochs: 1 / 10 Training Loss: 7.429 \n",
            "Epochs: 1 / 10 Training Loss: 6.878 \n",
            "Epochs: 1 / 10 Training Loss: 7.112 \n",
            "Epochs: 1 / 10 Training Loss: 6.814 \n",
            "Epochs: 1 / 10 Training Loss: 6.549 \n",
            "Epochs: 1 / 10 Training Loss: 6.828 \n",
            "Epochs: 1 / 10 Training Loss: 6.887 \n",
            "Epochs: 1 / 10 Training Loss: 7.398 \n",
            "Epochs: 1 / 10 Training Loss: 7.279 \n",
            "Epochs: 1 / 10 Training Loss: 6.915 \n",
            "Epochs: 1 / 10 Training Loss: 6.895 \n",
            "Epochs: 1 / 10 Training Loss: 6.349 \n",
            "Epochs: 1 / 10 Training Loss: 6.858 \n",
            "Epochs: 1 / 10 Training Loss: 6.696 \n",
            "Epochs: 1 / 10 Training Loss: 7.524 \n",
            "Epochs: 1 / 10 Training Loss: 7.080 \n",
            "Epochs: 1 / 10 Training Loss: 6.543 \n",
            "Epochs: 1 / 10 Training Loss: 7.004 \n",
            "Epochs: 1 / 10 Training Loss: 6.735 \n",
            "Epochs: 1 / 10 Training Loss: 6.634 \n",
            "Epochs: 1 / 10 Training Loss: 6.773 \n",
            "Epochs: 1 / 10 Training Loss: 7.188 \n",
            "Epochs: 1 / 10 Training Loss: 7.477 \n",
            "Epochs: 1 / 10 Training Loss: 7.491 \n",
            "Epochs: 1 / 10 Training Loss: 7.237 \n",
            "Epochs: 1 / 10 Training Loss: 7.009 \n",
            "Epochs: 1 / 10 Training Loss: 7.605 \n",
            "Epochs: 1 / 10 Training Loss: 6.634 \n",
            "Epochs: 1 / 10 Training Loss: 6.802 \n",
            "Epochs: 1 / 10 Training Loss: 6.282 \n",
            "Epochs: 1 / 10 Training Loss: 6.752 \n",
            "Epochs: 1 / 10 Training Loss: 7.710 \n",
            "Epochs: 1 / 10 Training Loss: 6.224 \n",
            "Epochs: 1 / 10 Training Loss: 6.007 \n",
            "Epochs: 1 / 10 Training Loss: 6.592 \n",
            "Epochs: 1 / 10 Training Loss: 6.849 \n",
            "Epochs: 1 / 10 Training Loss: 7.250 \n",
            "Epochs: 1 / 10 Training Loss: 6.948 \n",
            "Epochs: 1 / 10 Training Loss: 6.282 \n",
            "Epochs: 1 / 10 Training Loss: 6.405 \n",
            "Epochs: 1 / 10 Training Loss: 6.850 \n",
            "Epochs: 1 / 10 Training Loss: 6.771 \n",
            "Epochs: 1 / 10 Training Loss: 7.016 \n",
            "Epochs: 1 / 10 Training Loss: 7.078 \n",
            "Epochs: 1 / 10 Training Loss: 6.376 \n",
            "Epochs: 1 / 10 Training Loss: 7.442 \n",
            "Epochs: 1 / 10 Training Loss: 7.021 \n",
            "Epochs: 1 / 10 Training Loss: 6.628 \n",
            "Epochs: 1 / 10 Training Loss: 6.919 \n",
            "Epochs: 1 / 10 Training Loss: 6.989 \n",
            "Epochs: 1 / 10 Training Loss: 6.365 \n",
            "Epochs: 1 / 10 Training Loss: 6.502 \n",
            "Epochs: 1 / 10 Training Loss: 7.204 \n",
            "Epochs: 1 / 10 Training Loss: 7.250 \n",
            "Epochs: 1 / 10 Training Loss: 6.577 \n",
            "Epochs: 1 / 10 Training Loss: 6.741 \n",
            "Epochs: 1 / 10 Training Loss: 6.699 \n",
            "Epochs: 1 / 10 Training Loss: 6.068 \n",
            "Epochs: 1 / 10 Training Loss: 6.678 \n",
            "Epochs: 1 / 10 Training Loss: 6.406 \n",
            "Epochs: 1 / 10 Training Loss: 6.516 \n",
            "Epochs: 1 / 10 Training Loss: 6.014 \n",
            "Epochs: 1 / 10 Training Loss: 7.329 \n",
            "Epochs: 1 / 10 Training Loss: 6.539 \n",
            "Epochs: 1 / 10 Training Loss: 6.881 \n",
            "Epochs: 1 / 10 Training Loss: 5.809 \n",
            "Epochs: 1 / 10 Training Loss: 6.451 \n",
            "Epochs: 1 / 10 Training Loss: 7.275 \n",
            "Epochs: 1 / 10 Training Loss: 7.399 \n",
            "Epochs: 1 / 10 Training Loss: 6.787 \n",
            "Epochs: 1 / 10 Training Loss: 6.482 \n",
            "Epochs: 1 / 10 Training Loss: 6.594 \n",
            "Epochs: 1 / 10 Training Loss: 6.827 \n",
            "Epochs: 1 / 10 Training Loss: 6.618 \n",
            "Epochs: 1 / 10 Training Loss: 7.147 \n",
            "Epochs: 1 / 10 Training Loss: 6.588 \n",
            "Epochs: 1 / 10 Training Loss: 6.576 \n",
            "Epochs: 1 / 10 Training Loss: 6.468 \n",
            "Epochs: 1 / 10 Training Loss: 6.197 \n",
            "Epochs: 1 / 10 Training Loss: 5.825 \n",
            "Epochs: 1 / 10 Training Loss: 6.834 \n",
            "Epochs: 1 / 10 Training Loss: 5.929 \n",
            "Epochs: 1 / 10 Training Loss: 7.395 \n",
            "Epochs: 1 / 10 Training Loss: 6.512 \n",
            "Epochs: 1 / 10 Training Loss: 6.633 \n",
            "Epochs: 1 / 10 Training Loss: 6.585 \n",
            "Epochs: 1 / 10 Training Loss: 7.272 \n",
            "Epochs: 1 / 10 Training Loss: 6.696 \n",
            "Epochs: 1 / 10 Training Loss: 7.075 \n",
            "Epochs: 1 / 10 Training Loss: 6.535 \n",
            "Epochs: 1 / 10 Training Loss: 6.461 \n",
            "Epochs: 1 / 10 Training Loss: 6.250 \n",
            "Epochs: 1 / 10 Training Loss: 6.882 \n",
            "Epochs: 1 / 10 Training Loss: 7.099 \n",
            "Epochs: 1 / 10 Training Loss: 6.553 \n",
            "Epochs: 1 / 10 Training Loss: 6.245 \n",
            "Epochs: 1 / 10 Training Loss: 6.995 \n",
            "Epochs: 1 / 10 Training Loss: 6.958 \n",
            "Epochs: 1 / 10 Training Loss: 6.239 \n",
            "Epochs: 1 / 10 Training Loss: 6.254 \n",
            "Epochs: 1 / 10 Training Loss: 6.586 \n",
            "Epochs: 1 / 10 Training Loss: 7.031 \n",
            "Epochs: 1 / 10 Training Loss: 6.636 \n",
            "Epochs: 1 / 10 Training Loss: 6.247 \n",
            "Epochs: 1 / 10 Training Loss: 6.300 \n",
            "Epochs: 1 / 10 Training Loss: 6.404 \n",
            "Epochs: 1 / 10 Training Loss: 6.529 \n",
            "Epochs: 1 / 10 Training Loss: 6.586 \n",
            "Epochs: 1 / 10 Training Loss: 6.891 \n",
            "Epochs: 1 / 10 Training Loss: 6.253 \n",
            "Epochs: 1 / 10 Training Loss: 6.063 \n",
            "Epochs: 1 / 10 Training Loss: 6.168 \n",
            "Epochs: 1 / 10 Training Loss: 7.190 \n",
            "Epochs: 1 / 10 Training Loss: 5.859 \n",
            "Epochs: 1 / 10 Training Loss: 6.699 \n",
            "Epochs: 1 / 10 Training Loss: 5.573 \n",
            "Epochs: 1 / 10 Training Loss: 6.661 \n",
            "Epochs: 1 / 10 Training Loss: 6.533 \n",
            "Epochs: 1 / 10 Training Loss: 6.013 \n",
            "Epochs: 1 / 10 Training Loss: 6.434 \n",
            "Epochs: 1 / 10 Training Loss: 7.295 \n",
            "Epochs: 1 / 10 Training Loss: 6.575 \n",
            "Epochs: 1 / 10 Training Loss: 6.358 \n",
            "Epochs: 1 / 10 Training Loss: 5.921 \n",
            "Epochs: 1 / 10 Training Loss: 7.013 \n",
            "Epochs: 1 / 10 Training Loss: 6.390 \n",
            "Epochs: 1 / 10 Training Loss: 6.217 \n",
            "Epochs: 1 / 10 Training Loss: 7.416 \n",
            "Epochs: 1 / 10 Training Loss: 6.722 \n",
            "Epochs: 1 / 10 Training Loss: 6.705 \n",
            "Epochs: 1 / 10 Training Loss: 6.497 \n",
            "Epochs: 1 / 10 Training Loss: 6.216 \n",
            "Epochs: 1 / 10 Training Loss: 6.055 \n",
            "Epochs: 1 / 10 Training Loss: 6.245 \n",
            "Epochs: 1 / 10 Training Loss: 7.225 \n",
            "Epochs: 1 / 10 Training Loss: 7.636 \n",
            "Epochs: 1 / 10 Training Loss: 7.565 \n",
            "Epochs: 1 / 10 Training Loss: 7.343 \n",
            "Epochs: 1 / 10 Training Loss: 7.152 \n",
            "Epochs: 1 / 10 Training Loss: 7.561 \n",
            "Epochs: 1 / 10 Training Loss: 7.102 \n",
            "Epochs: 1 / 10 Training Loss: 6.409 \n",
            "Epochs: 1 / 10 Training Loss: 6.858 \n",
            "Epochs: 1 / 10 Training Loss: 6.294 \n",
            "Epochs: 1 / 10 Training Loss: 6.785 \n",
            "Epochs: 1 / 10 Training Loss: 6.936 \n",
            "Epochs: 1 / 10 Training Loss: 6.521 \n",
            "Epochs: 1 / 10 Training Loss: 6.477 \n",
            "Epochs: 1 / 10 Training Loss: 6.378 \n",
            "Epochs: 1 / 10 Training Loss: 6.369 \n",
            "Epochs: 1 / 10 Training Loss: 6.809 \n",
            "Epochs: 1 / 10 Training Loss: 6.384 \n",
            "Epochs: 1 / 10 Training Loss: 7.037 \n",
            "Epochs: 1 / 10 Training Loss: 7.222 \n",
            "Epochs: 1 / 10 Training Loss: 7.317 \n",
            "Epochs: 1 / 10 Training Loss: 6.994 \n",
            "Epochs: 1 / 10 Training Loss: 6.985 \n",
            "Epochs: 1 / 10 Training Loss: 6.662 \n",
            "Epochs: 1 / 10 Training Loss: 6.938 \n",
            "Epochs: 1 / 10 Training Loss: 6.828 \n",
            "Epochs: 1 / 10 Training Loss: 7.155 \n",
            "Epochs: 1 / 10 Training Loss: 6.642 \n",
            "Epochs: 1 / 10 Training Loss: 6.727 \n",
            "Epochs: 1 / 10 Training Loss: 6.392 \n",
            "Epochs: 1 / 10 Training Loss: 6.777 \n",
            "Epochs: 1 / 10 Training Loss: 7.607 \n",
            "Epochs: 1 / 10 Training Loss: 6.672 \n",
            "Epochs: 1 / 10 Training Loss: 6.586 \n",
            "Epochs: 1 / 10 Training Loss: 7.127 \n",
            "Epochs: 1 / 10 Training Loss: 7.025 \n",
            "Epochs: 1 / 10 Training Loss: 5.859 \n",
            "Epochs: 1 / 10 Training Loss: 7.103 \n",
            "Epochs: 1 / 10 Training Loss: 6.605 \n",
            "Epochs: 1 / 10 Training Loss: 6.304 \n",
            "Epochs: 1 / 10 Training Loss: 6.311 \n",
            "Epochs: 1 / 10 Training Loss: 6.338 \n",
            "Epochs: 1 / 10 Training Loss: 6.662 \n",
            "Epochs: 1 / 10 Training Loss: 6.441 \n",
            "Epochs: 1 / 10 Training Loss: 6.452 \n",
            "Epochs: 1 / 10 Training Loss: 6.650 \n",
            "Epochs: 1 / 10 Training Loss: 7.036 \n",
            "Epochs: 1 / 10 Training Loss: 6.987 \n",
            "Epochs: 1 / 10 Training Loss: 7.210 \n",
            "Epochs: 1 / 10 Training Loss: 6.705 \n",
            "Epochs: 1 / 10 Training Loss: 6.661 \n",
            "Epochs: 1 / 10 Training Loss: 6.462 \n",
            "Epochs: 1 / 10 Training Loss: 6.692 \n",
            "Epochs: 1 / 10 Training Loss: 7.039 \n",
            "Epochs: 1 / 10 Training Loss: 7.080 \n",
            "Epochs: 1 / 10 Training Loss: 6.792 \n",
            "Epochs: 1 / 10 Training Loss: 7.053 \n",
            "Epochs: 1 / 10 Training Loss: 5.915 \n",
            "Epochs: 1 / 10 Training Loss: 6.639 \n",
            "Epochs: 1 / 10 Training Loss: 6.584 \n",
            "Epochs: 1 / 10 Training Loss: 6.930 \n",
            "Epochs: 1 / 10 Training Loss: 6.910 \n",
            "Epochs: 1 / 10 Training Loss: 7.345 \n",
            "Epochs: 1 / 10 Training Loss: 7.680 \n",
            "Epochs: 1 / 10 Training Loss: 6.203 \n",
            "Epochs: 1 / 10 Training Loss: 5.636 \n",
            "Epochs: 1 / 10 Training Loss: 7.230 \n",
            "Epochs: 1 / 10 Training Loss: 7.014 \n",
            "Epochs: 1 / 10 Training Loss: 6.599 \n",
            "Epochs: 1 / 10 Training Loss: 7.084 \n",
            "Epochs: 1 / 10 Training Loss: 7.009 \n",
            "Epochs: 1 / 10 Training Loss: 6.031 \n",
            "Epochs: 1 / 10 Training Loss: 7.066 \n",
            "Epochs: 1 / 10 Training Loss: 6.803 \n",
            "Epochs: 1 / 10 Training Loss: 5.954 \n",
            "Epochs: 1 / 10 Training Loss: 6.684 \n",
            "Epochs: 1 / 10 Training Loss: 7.151 \n",
            "Epochs: 1 / 10 Training Loss: 6.918 \n",
            "Epochs: 1 / 10 Training Loss: 6.286 \n",
            "Epochs: 1 / 10 Training Loss: 6.345 \n",
            "Epochs: 1 / 10 Training Loss: 6.160 \n",
            "Epochs: 1 / 10 Training Loss: 6.868 \n",
            "Epochs: 1 / 10 Training Loss: 6.461 \n",
            "Epochs: 1 / 10 Training Loss: 6.071 \n",
            "Epochs: 1 / 10 Training Loss: 6.941 \n",
            "Epochs: 1 / 10 Training Loss: 6.986 \n",
            "Epochs: 1 / 10 Training Loss: 6.804 \n",
            "Epochs: 1 / 10 Training Loss: 6.887 \n",
            "Epochs: 1 / 10 Training Loss: 6.400 \n",
            "Epochs: 1 / 10 Training Loss: 6.214 \n",
            "Epochs: 1 / 10 Training Loss: 7.066 \n",
            "Epochs: 1 / 10 Training Loss: 6.568 \n",
            "Epochs: 1 / 10 Training Loss: 7.276 \n",
            "Epochs: 1 / 10 Training Loss: 6.985 \n",
            "Epochs: 1 / 10 Training Loss: 7.506 \n",
            "Epochs: 1 / 10 Training Loss: 6.552 \n",
            "Epochs: 1 / 10 Training Loss: 6.358 \n",
            "Epochs: 1 / 10 Training Loss: 6.815 \n",
            "Epochs: 1 / 10 Training Loss: 6.620 \n",
            "Epochs: 1 / 10 Training Loss: 6.581 \n",
            "Epochs: 1 / 10 Training Loss: 6.436 \n",
            "Epochs: 1 / 10 Training Loss: 6.334 \n",
            "Epochs: 1 / 10 Training Loss: 6.974 \n",
            "Epochs: 1 / 10 Training Loss: 6.857 \n",
            "Epochs: 1 / 10 Training Loss: 6.907 \n",
            "Epochs: 1 / 10 Training Loss: 7.253 \n",
            "Epochs: 1 / 10 Training Loss: 6.198 \n",
            "Epochs: 1 / 10 Training Loss: 6.513 \n",
            "Epochs: 1 / 10 Training Loss: 6.437 \n",
            "Epochs: 1 / 10 Training Loss: 6.380 \n",
            "Epochs: 1 / 10 Training Loss: 6.422 \n",
            "Epochs: 1 / 10 Training Loss: 6.159 \n",
            "Epochs: 1 / 10 Training Loss: 6.479 \n",
            "Epochs: 1 / 10 Training Loss: 6.267 \n",
            "Epochs: 1 / 10 Training Loss: 6.578 \n",
            "Epochs: 1 / 10 Training Loss: 6.226 \n",
            "Epochs: 1 / 10 Training Loss: 6.436 \n",
            "Epochs: 1 / 10 Training Loss: 6.423 \n",
            "Epochs: 1 / 10 Training Loss: 6.316 \n",
            "Epochs: 1 / 10 Training Loss: 6.820 \n",
            "Epochs: 1 / 10 Training Loss: 6.187 \n",
            "Epochs: 1 / 10 Training Loss: 7.308 \n",
            "Epochs: 1 / 10 Training Loss: 6.415 \n",
            "Epochs: 1 / 10 Training Loss: 6.482 \n",
            "Epochs: 1 / 10 Training Loss: 6.343 \n",
            "Epochs: 1 / 10 Training Loss: 5.966 \n",
            "Epochs: 1 / 10 Training Loss: 6.622 \n",
            "Epochs: 1 / 10 Training Loss: 5.954 \n",
            "Epochs: 1 / 10 Training Loss: 6.981 \n",
            "Epochs: 1 / 10 Training Loss: 6.473 \n",
            "Epochs: 1 / 10 Training Loss: 6.700 \n",
            "Epochs: 1 / 10 Training Loss: 6.463 \n",
            "Epochs: 1 / 10 Training Loss: 6.163 \n",
            "Epochs: 1 / 10 Training Loss: 6.484 \n",
            "Epochs: 1 / 10 Training Loss: 6.724 \n",
            "Epochs: 1 / 10 Training Loss: 6.043 \n",
            "Epochs: 1 / 10 Training Loss: 5.932 \n",
            "Epochs: 1 / 10 Training Loss: 6.076 \n",
            "Epochs: 1 / 10 Training Loss: 6.260 \n",
            "Epochs: 1 / 10 Training Loss: 6.835 \n",
            "Epochs: 1 / 10 Training Loss: 6.888 \n",
            "Epochs: 1 / 10 Training Loss: 6.365 \n",
            "Epochs: 1 / 10 Training Loss: 6.008 \n",
            "Epochs: 1 / 10 Training Loss: 6.114 \n",
            "Epochs: 1 / 10 Training Loss: 5.776 \n",
            "Epochs: 1 / 10 Training Loss: 6.196 \n",
            "Epochs: 1 / 10 Training Loss: 6.159 \n",
            "Epochs: 1 / 10 Training Loss: 6.454 \n",
            "Epochs: 1 / 10 Training Loss: 6.918 \n",
            "Epochs: 1 / 10 Training Loss: 6.401 \n",
            "Epochs: 1 / 10 Training Loss: 6.069 \n",
            "Epochs: 1 / 10 Training Loss: 6.261 \n",
            "Epochs: 1 / 10 Training Loss: 5.661 \n",
            "Epochs: 1 / 10 Training Loss: 6.169 \n",
            "Epochs: 1 / 10 Training Loss: 6.257 \n",
            "Epochs: 1 / 10 Training Loss: 6.222 \n",
            "Epochs: 1 / 10 Training Loss: 6.990 \n",
            "Epochs: 1 / 10 Training Loss: 6.993 \n",
            "Epochs: 1 / 10 Training Loss: 6.645 \n",
            "Epochs: 1 / 10 Training Loss: 6.333 \n",
            "Epochs: 1 / 10 Training Loss: 6.264 \n",
            "Epochs: 1 / 10 Training Loss: 5.874 \n",
            "Epochs: 1 / 10 Training Loss: 6.672 \n",
            "Epochs: 1 / 10 Training Loss: 6.514 \n",
            "Epochs: 1 / 10 Training Loss: 5.744 \n",
            "Epochs: 1 / 10 Training Loss: 6.909 \n",
            "Epochs: 1 / 10 Training Loss: 7.187 \n",
            "Epochs: 1 / 10 Training Loss: 6.512 \n",
            "Epochs: 1 / 10 Training Loss: 7.566 \n",
            "Epochs: 1 / 10 Training Loss: 6.085 \n",
            "Epochs: 1 / 10 Training Loss: 6.074 \n",
            "Epochs: 1 / 10 Training Loss: 6.540 \n",
            "Epochs: 1 / 10 Training Loss: 5.732 \n",
            "Epochs: 1 / 10 Training Loss: 6.266 \n",
            "Epochs: 1 / 10 Training Loss: 6.334 \n",
            "Epochs: 1 / 10 Training Loss: 5.544 \n",
            "Epochs: 1 / 10 Training Loss: 6.526 \n",
            "Epochs: 1 / 10 Training Loss: 5.655 \n",
            "Epochs: 1 / 10 Training Loss: 5.777 \n",
            "Epochs: 1 / 10 Training Loss: 5.990 \n",
            "Epochs: 1 / 10 Training Loss: 6.127 \n",
            "Epochs: 1 / 10 Training Loss: 6.147 \n",
            "Epochs: 1 / 10 Training Loss: 6.254 \n",
            "Epochs: 1 / 10 Training Loss: 5.743 \n",
            "Epochs: 1 / 10 Training Loss: 7.043 \n",
            "Epochs: 1 / 10 Training Loss: 4.665 \n",
            "Epochs: 1 / 10 Training Loss: 5.751 \n",
            "Epochs: 1 / 10 Training Loss: 6.169 \n",
            "Epochs: 1 / 10 Training Loss: 6.166 \n",
            "Epochs: 1 / 10 Training Loss: 6.144 \n",
            "Epochs: 1 / 10 Training Loss: 5.739 \n",
            "Epochs: 1 / 10 Training Loss: 6.559 \n",
            "Epochs: 1 / 10 Training Loss: 6.079 \n",
            "Epochs: 1 / 10 Training Loss: 6.200 \n",
            "Epochs: 1 / 10 Training Loss: 6.361 \n",
            "Epochs: 1 / 10 Training Loss: 6.411 \n",
            "Epochs: 1 / 10 Training Loss: 6.508 \n",
            "Epochs: 1 / 10 Training Loss: 6.035 \n",
            "Epochs: 1 / 10 Training Loss: 6.104 \n",
            "Epochs: 1 / 10 Training Loss: 6.040 \n",
            "Epochs: 1 / 10 Training Loss: 5.880 \n",
            "Epochs: 1 / 10 Training Loss: 6.085 \n",
            "Epochs: 1 / 10 Training Loss: 6.390 \n",
            "Epochs: 1 / 10 Training Loss: 5.891 \n",
            "Epochs: 1 / 10 Training Loss: 5.942 \n",
            "Epochs: 1 / 10 Training Loss: 6.729 \n",
            "Epochs: 1 / 10 Training Loss: 7.082 \n",
            "Epochs: 1 / 10 Training Loss: 6.350 \n",
            "Epochs: 1 / 10 Training Loss: 7.298 \n",
            "Epochs: 1 / 10 Training Loss: 7.319 \n",
            "Epochs: 1 / 10 Training Loss: 6.058 \n",
            "Epochs: 1 / 10 Training Loss: 6.216 \n",
            "Epochs: 1 / 10 Training Loss: 6.360 \n",
            "Epochs: 1 / 10 Training Loss: 6.570 \n",
            "Epochs: 1 / 10 Training Loss: 6.207 \n",
            "Epochs: 1 / 10 Training Loss: 6.202 \n",
            "Epochs: 1 / 10 Training Loss: 6.189 \n",
            "Epochs: 1 / 10 Training Loss: 7.272 \n",
            "Epochs: 1 / 10 Training Loss: 6.383 \n",
            "Epochs: 1 / 10 Training Loss: 6.261 \n",
            "Epochs: 1 / 10 Training Loss: 5.825 \n",
            "Epochs: 1 / 10 Training Loss: 6.070 \n",
            "Epochs: 1 / 10 Training Loss: 5.984 \n",
            "Epochs: 1 / 10 Training Loss: 5.903 \n",
            "Epochs: 1 / 10 Training Loss: 5.909 \n",
            "Epochs: 1 / 10 Training Loss: 5.989 \n",
            "Epochs: 1 / 10 Training Loss: 6.051 \n",
            "Epochs: 1 / 10 Training Loss: 5.880 \n",
            "Epochs: 1 / 10 Training Loss: 5.965 \n",
            "Epochs: 1 / 10 Training Loss: 5.748 \n",
            "Epochs: 1 / 10 Training Loss: 5.993 \n",
            "Epochs: 1 / 10 Training Loss: 6.385 \n",
            "Epochs: 1 / 10 Training Loss: 6.096 \n",
            "Epochs: 1 / 10 Training Loss: 6.447 \n",
            "Epochs: 1 / 10 Training Loss: 6.651 \n",
            "Epochs: 1 / 10 Training Loss: 5.506 \n",
            "Epochs: 1 / 10 Training Loss: 5.836 \n",
            "Epochs: 1 / 10 Training Loss: 5.944 \n",
            "Epochs: 1 / 10 Training Loss: 6.486 \n",
            "Epochs: 1 / 10 Training Loss: 6.002 \n",
            "Epochs: 1 / 10 Training Loss: 6.352 \n",
            "Epochs: 1 / 10 Training Loss: 5.946 \n",
            "Epochs: 1 / 10 Training Loss: 5.948 \n",
            "Epochs: 1 / 10 Training Loss: 6.293 \n",
            "Epochs: 1 / 10 Training Loss: 5.908 \n",
            "Epochs: 1 / 10 Training Loss: 6.257 \n",
            "Epochs: 1 / 10 Training Loss: 6.263 \n",
            "Epochs: 1 / 10 Training Loss: 5.612 \n",
            "Epochs: 1 / 10 Training Loss: 5.884 \n",
            "Epochs: 1 / 10 Training Loss: 6.671 \n",
            "Epochs: 1 / 10 Training Loss: 6.658 \n",
            "Epochs: 1 / 10 Training Loss: 5.738 \n",
            "Epochs: 1 / 10 Training Loss: 6.107 \n",
            "Epochs: 1 / 10 Training Loss: 6.005 \n",
            "Epochs: 1 / 10 Training Loss: 6.781 \n",
            "Epochs: 1 / 10 Training Loss: 5.967 \n",
            "Epochs: 1 / 10 Training Loss: 5.966 \n",
            "Epochs: 1 / 10 Training Loss: 6.204 \n",
            "Epochs: 1 / 10 Training Loss: 6.374 \n",
            "Epochs: 1 / 10 Training Loss: 4.855 \n",
            "Epochs: 1 / 10 Training Loss: 5.191 \n",
            "Epochs: 1 / 10 Training Loss: 5.398 \n",
            "Epochs: 1 / 10 Training Loss: 5.972 \n",
            "Epochs: 1 / 10 Training Loss: 5.867 \n",
            "Epochs: 1 / 10 Training Loss: 5.966 \n",
            "Epochs: 1 / 10 Training Loss: 5.859 \n",
            "Epochs: 1 / 10 Training Loss: 5.780 \n",
            "Epochs: 1 / 10 Training Loss: 7.138 \n",
            "Epochs: 1 / 10 Training Loss: 5.908 \n",
            "Epochs: 1 / 10 Training Loss: 6.919 \n",
            "Epochs: 1 / 10 Training Loss: 6.024 \n",
            "Epochs: 1 / 10 Training Loss: 7.493 \n",
            "Epochs: 1 / 10 Training Loss: 6.866 \n",
            "Epochs: 1 / 10 Training Loss: 6.865 \n",
            "Epochs: 1 / 10 Training Loss: 5.880 \n",
            "Epochs: 1 / 10 Training Loss: 5.832 \n",
            "Epochs: 1 / 10 Training Loss: 6.643 \n",
            "Epochs: 1 / 10 Training Loss: 5.886 \n",
            "Epochs: 1 / 10 Training Loss: 6.127 \n",
            "Epochs: 1 / 10 Training Loss: 6.511 \n",
            "Epochs: 1 / 10 Training Loss: 6.157 \n",
            "Epochs: 1 / 10 Training Loss: 6.085 \n",
            "Epochs: 1 / 10 Training Loss: 6.351 \n",
            "Epochs: 1 / 10 Training Loss: 5.928 \n",
            "Epochs: 1 / 10 Training Loss: 5.929 \n",
            "Epochs: 1 / 10 Training Loss: 5.364 \n",
            "Epochs: 1 / 10 Training Loss: 5.483 \n",
            "Epochs: 1 / 10 Training Loss: 6.147 \n",
            "Epochs: 1 / 10 Training Loss: 6.035 \n",
            "Epochs: 1 / 10 Training Loss: 5.547 \n",
            "Epochs: 1 / 10 Training Loss: 6.164 \n",
            "Epochs: 1 / 10 Training Loss: 6.023 \n",
            "Epochs: 1 / 10 Training Loss: 6.233 \n",
            "Epochs: 1 / 10 Training Loss: 6.441 \n",
            "Epochs: 1 / 10 Training Loss: 5.721 \n",
            "Epochs: 1 / 10 Training Loss: 6.072 \n",
            "Epochs: 1 / 10 Training Loss: 6.275 \n",
            "Epochs: 1 / 10 Training Loss: 6.639 \n",
            "Epochs: 1 / 10 Training Loss: 5.681 \n",
            "Epochs: 1 / 10 Training Loss: 5.934 \n",
            "Epochs: 1 / 10 Training Loss: 6.023 \n",
            "Epochs: 1 / 10 Training Loss: 5.868 \n",
            "Epochs: 1 / 10 Training Loss: 6.317 \n",
            "Epochs: 1 / 10 Training Loss: 5.995 \n",
            "Epochs: 1 / 10 Training Loss: 5.661 \n",
            "Epochs: 1 / 10 Training Loss: 5.964 \n",
            "Epochs: 1 / 10 Training Loss: 6.326 \n",
            "Epochs: 1 / 10 Training Loss: 6.246 \n",
            "Epochs: 1 / 10 Training Loss: 6.194 \n",
            "Epochs: 1 / 10 Training Loss: 6.466 \n",
            "Epochs: 1 / 10 Training Loss: 6.813 \n",
            "Epochs: 1 / 10 Training Loss: 6.731 \n",
            "Epochs: 1 / 10 Training Loss: 6.345 \n",
            "Epochs: 1 / 10 Training Loss: 6.917 \n",
            "Epochs: 1 / 10 Training Loss: 6.828 \n",
            "Epochs: 1 / 10 Training Loss: 6.505 \n",
            "Epochs: 1 / 10 Training Loss: 6.429 \n",
            "Epochs: 1 / 10 Training Loss: 6.399 \n",
            "Epochs: 1 / 10 Training Loss: 6.342 \n",
            "Epochs: 1 / 10 Training Loss: 5.983 \n",
            "Epochs: 1 / 10 Training Loss: 5.895 \n",
            "Epochs: 1 / 10 Training Loss: 6.090 \n",
            "Epochs: 1 / 10 Training Loss: 6.702 \n",
            "Epochs: 1 / 10 Training Loss: 6.083 \n",
            "Epochs: 1 / 10 Training Loss: 5.732 \n",
            "Epochs: 1 / 10 Training Loss: 6.500 \n",
            "Epochs: 1 / 10 Training Loss: 6.011 \n",
            "Epochs: 1 / 10 Training Loss: 5.801 \n",
            "Epochs: 1 / 10 Training Loss: 5.915 \n",
            "Epochs: 1 / 10 Training Loss: 5.776 \n",
            "Epochs: 1 / 10 Training Loss: 6.177 \n",
            "Epochs: 1 / 10 Training Loss: 5.211 \n",
            "Epochs: 1 / 10 Training Loss: 5.676 \n",
            "Epochs: 1 / 10 Training Loss: 5.445 \n",
            "Epochs: 1 / 10 Training Loss: 5.936 \n",
            "Epochs: 1 / 10 Training Loss: 6.031 \n",
            "Epochs: 1 / 10 Training Loss: 5.375 \n",
            "Epochs: 1 / 10 Training Loss: 6.202 \n",
            "Epochs: 1 / 10 Training Loss: 5.522 \n",
            "Epochs: 1 / 10 Training Loss: 5.855 \n",
            "Epochs: 1 / 10 Training Loss: 7.109 \n",
            "Epochs: 1 / 10 Training Loss: 7.077 \n",
            "Epochs: 1 / 10 Training Loss: 6.726 \n",
            "Epochs: 1 / 10 Training Loss: 6.389 \n",
            "Epochs: 1 / 10 Training Loss: 7.338 \n",
            "Epochs: 1 / 10 Training Loss: 6.567 \n",
            "Epochs: 1 / 10 Training Loss: 7.089 \n",
            "Epochs: 1 / 10 Training Loss: 6.248 \n",
            "Epochs: 1 / 10 Training Loss: 6.370 \n",
            "Epochs: 1 / 10 Training Loss: 6.359 \n",
            "Epochs: 1 / 10 Training Loss: 6.324 \n",
            "Epochs: 1 / 10 Training Loss: 6.295 \n",
            "Epochs: 1 / 10 Training Loss: 6.631 \n",
            "Epochs: 1 / 10 Training Loss: 6.420 \n",
            "Epochs: 1 / 10 Training Loss: 6.087 \n",
            "Epochs: 1 / 10 Training Loss: 6.264 \n",
            "Epochs: 1 / 10 Training Loss: 5.939 \n",
            "Epochs: 1 / 10 Training Loss: 6.099 \n",
            "Epochs: 1 / 10 Training Loss: 6.721 \n",
            "Epochs: 1 / 10 Training Loss: 6.321 \n",
            "Epochs: 1 / 10 Training Loss: 5.020 \n",
            "Epochs: 1 / 10 Training Loss: 5.783 \n",
            "Epochs: 1 / 10 Training Loss: 6.290 \n",
            "Epochs: 1 / 10 Training Loss: 6.152 \n",
            "Epochs: 1 / 10 Training Loss: 6.879 \n",
            "Epochs: 1 / 10 Training Loss: 6.958 \n",
            "Epochs: 1 / 10 Training Loss: 6.384 \n",
            "Epochs: 1 / 10 Training Loss: 6.638 \n",
            "Epochs: 1 / 10 Training Loss: 6.302 \n",
            "Epochs: 1 / 10 Training Loss: 8.268 \n",
            "Epochs: 1 / 10 Training Loss: 6.298 \n",
            "Epochs: 1 / 10 Training Loss: 6.319 \n",
            "Epochs: 1 / 10 Training Loss: 6.073 \n",
            "Epochs: 1 / 10 Training Loss: 6.084 \n",
            "Epochs: 1 / 10 Training Loss: 5.772 \n",
            "Epochs: 1 / 10 Training Loss: 5.481 \n",
            "Epochs: 1 / 10 Training Loss: 6.849 \n",
            "Epochs: 1 / 10 Training Loss: 6.445 \n",
            "Epochs: 1 / 10 Training Loss: 5.881 \n",
            "Epochs: 1 / 10 Training Loss: 5.996 \n",
            "Epochs: 1 / 10 Training Loss: 6.149 \n",
            "Epochs: 1 / 10 Training Loss: 6.668 \n",
            "Epochs: 1 / 10 Training Loss: 5.920 \n",
            "Epochs: 1 / 10 Training Loss: 5.772 \n",
            "Epochs: 1 / 10 Training Loss: 6.367 \n",
            "Epochs: 1 / 10 Training Loss: 5.701 \n",
            "Epochs: 1 / 10 Training Loss: 6.719 \n",
            "Epochs: 1 / 10 Training Loss: 6.103 \n",
            "Epochs: 1 / 10 Training Loss: 6.293 \n",
            "Epochs: 1 / 10 Training Loss: 6.169 \n",
            "Epochs: 1 / 10 Training Loss: 5.981 \n",
            "Epochs: 1 / 10 Training Loss: 6.545 \n",
            "Epochs: 1 / 10 Training Loss: 5.678 \n",
            "Epochs: 1 / 10 Training Loss: 5.714 \n",
            "Epochs: 1 / 10 Training Loss: 5.797 \n",
            "Epochs: 1 / 10 Training Loss: 6.128 \n",
            "Epochs: 1 / 10 Training Loss: 6.233 \n",
            "Epochs: 1 / 10 Training Loss: 5.922 \n",
            "Epochs: 1 / 10 Training Loss: 6.074 \n",
            "Epochs: 1 / 10 Training Loss: 6.177 \n",
            "Epochs: 1 / 10 Training Loss: 6.704 \n",
            "Epochs: 1 / 10 Training Loss: 6.564 \n",
            "Epochs: 1 / 10 Training Loss: 6.890 \n",
            "Epochs: 1 / 10 Training Loss: 6.202 \n",
            "Epochs: 1 / 10 Training Loss: 6.770 \n",
            "Epochs: 1 / 10 Training Loss: 7.100 \n",
            "Epochs: 1 / 10 Training Loss: 5.609 \n",
            "Epochs: 1 / 10 Training Loss: 6.377 \n",
            "Epochs: 1 / 10 Training Loss: 6.710 \n",
            "Epochs: 1 / 10 Training Loss: 5.658 \n",
            "Epochs: 1 / 10 Training Loss: 7.004 \n",
            "Epochs: 1 / 10 Training Loss: 6.281 \n",
            "Epochs: 1 / 10 Training Loss: 6.128 \n",
            "Epochs: 1 / 10 Training Loss: 5.971 \n",
            "Epochs: 1 / 10 Training Loss: 6.703 \n",
            "Epochs: 1 / 10 Training Loss: 6.429 \n",
            "Epochs: 1 / 10 Training Loss: 6.205 \n",
            "Epochs: 1 / 10 Training Loss: 5.834 \n",
            "Epochs: 1 / 10 Training Loss: 5.719 \n",
            "Epochs: 1 / 10 Training Loss: 5.667 \n",
            "Epochs: 1 / 10 Training Loss: 6.139 \n",
            "Epochs: 1 / 10 Training Loss: 6.062 \n",
            "Epochs: 1 / 10 Training Loss: 6.090 \n",
            "Epochs: 1 / 10 Training Loss: 5.133 \n",
            "Epochs: 1 / 10 Training Loss: 5.813 \n",
            "Epochs: 1 / 10 Training Loss: 6.037 \n",
            "Epochs: 1 / 10 Training Loss: 7.082 \n",
            "Epochs: 1 / 10 Training Loss: 5.943 \n",
            "Epochs: 1 / 10 Training Loss: 5.714 \n",
            "Epochs: 1 / 10 Training Loss: 5.164 \n",
            "Epochs: 1 / 10 Training Loss: 5.908 \n",
            "Epochs: 1 / 10 Training Loss: 6.705 \n",
            "Epochs: 1 / 10 Training Loss: 5.668 \n",
            "Epochs: 1 / 10 Training Loss: 6.332 \n",
            "Epochs: 1 / 10 Training Loss: 5.807 \n",
            "Epochs: 1 / 10 Training Loss: 6.491 \n",
            "Epochs: 1 / 10 Training Loss: 6.049 \n",
            "Epochs: 1 / 10 Training Loss: 5.766 \n",
            "Epochs: 1 / 10 Training Loss: 5.921 \n",
            "Epochs: 1 / 10 Training Loss: 5.962 \n",
            "Epochs: 1 / 10 Training Loss: 5.598 \n",
            "Epochs: 1 / 10 Training Loss: 6.379 \n",
            "Epochs: 1 / 10 Training Loss: 6.166 \n",
            "Epochs: 1 / 10 Training Loss: 5.752 \n",
            "Epochs: 1 / 10 Training Loss: 6.309 \n",
            "Epochs: 1 / 10 Training Loss: 6.142 \n",
            "Epochs: 1 / 10 Training Loss: 5.404 \n",
            "Epochs: 1 / 10 Training Loss: 6.016 \n",
            "Epochs: 1 / 10 Training Loss: 5.550 \n",
            "Epochs: 1 / 10 Training Loss: 5.878 \n",
            "Epochs: 1 / 10 Training Loss: 5.329 \n",
            "Epochs: 1 / 10 Training Loss: 5.356 \n",
            "Epochs: 1 / 10 Training Loss: 6.333 \n",
            "Epochs: 1 / 10 Training Loss: 5.559 \n",
            "Epochs: 1 / 10 Training Loss: 5.995 \n",
            "Epochs: 1 / 10 Training Loss: 5.589 \n",
            "Epochs: 1 / 10 Training Loss: 5.878 \n",
            "Epochs: 1 / 10 Training Loss: 5.681 \n",
            "Epochs: 1 / 10 Training Loss: 5.451 \n",
            "Epochs: 1 / 10 Training Loss: 5.508 \n",
            "Epochs: 1 / 10 Training Loss: 6.151 \n",
            "Epochs: 1 / 10 Training Loss: 4.958 \n",
            "Epochs: 1 / 10 Training Loss: 5.227 \n",
            "Epochs: 1 / 10 Training Loss: 5.376 \n",
            "Epochs: 1 / 10 Training Loss: 4.664 \n",
            "Epochs: 1 / 10 Training Loss: 5.254 \n",
            "Epochs: 1 / 10 Training Loss: 4.730 \n",
            "Epochs: 1 / 10 Training Loss: 6.310 \n",
            "Epochs: 1 / 10 Training Loss: 4.960 \n",
            "Epochs: 1 / 10 Training Loss: 5.880 \n",
            "Epochs: 1 / 10 Training Loss: 4.508 \n",
            "Epochs: 1 / 10 Training Loss: 5.782 \n",
            "Epochs: 1 / 10 Training Loss: 5.787 \n",
            "Epochs: 1 / 10 Training Loss: 4.965 \n",
            "Epochs: 1 / 10 Training Loss: 5.933 \n",
            "Epochs: 1 / 10 Training Loss: 5.270 \n",
            "Epochs: 1 / 10 Training Loss: 5.996 \n",
            "Epochs: 1 / 10 Training Loss: 6.274 \n",
            "Epochs: 1 / 10 Training Loss: 6.404 \n",
            "Epochs: 1 / 10 Training Loss: 7.038 \n",
            "Epochs: 1 / 10 Training Loss: 6.583 \n",
            "Epochs: 1 / 10 Training Loss: 6.557 \n",
            "Epochs: 1 / 10 Training Loss: 6.528 \n",
            "Epochs: 1 / 10 Training Loss: 5.981 \n",
            "Epochs: 1 / 10 Training Loss: 5.883 \n",
            "Epochs: 1 / 10 Training Loss: 6.127 \n",
            "Epochs: 1 / 10 Training Loss: 5.806 \n",
            "Epochs: 1 / 10 Training Loss: 5.935 \n",
            "Epochs: 1 / 10 Training Loss: 5.638 \n",
            "Epochs: 1 / 10 Training Loss: 5.714 \n",
            "Epochs: 1 / 10 Training Loss: 5.716 \n",
            "Epochs: 1 / 10 Training Loss: 4.819 \n",
            "Epochs: 1 / 10 Training Loss: 5.545 \n",
            "Epochs: 1 / 10 Training Loss: 5.352 \n",
            "Epochs: 1 / 10 Training Loss: 5.098 \n",
            "Epochs: 1 / 10 Training Loss: 5.750 \n",
            "Epochs: 1 / 10 Training Loss: 6.665 \n",
            "Epochs: 1 / 10 Training Loss: 6.623 \n",
            "Epochs: 1 / 10 Training Loss: 6.605 \n",
            "Epochs: 1 / 10 Training Loss: 5.951 \n",
            "Epochs: 1 / 10 Training Loss: 6.145 \n",
            "Epochs: 1 / 10 Training Loss: 5.565 \n",
            "Epochs: 1 / 10 Training Loss: 5.952 \n",
            "Epochs: 1 / 10 Training Loss: 5.890 \n",
            "Epochs: 1 / 10 Training Loss: 6.344 \n",
            "Epochs: 1 / 10 Training Loss: 5.494 \n",
            "Epochs: 1 / 10 Training Loss: 6.077 \n",
            "Epochs: 1 / 10 Training Loss: 5.919 \n",
            "Epochs: 1 / 10 Training Loss: 6.067 \n",
            "Epochs: 1 / 10 Training Loss: 6.159 \n",
            "Epochs: 1 / 10 Training Loss: 5.349 \n",
            "Epochs: 1 / 10 Training Loss: 6.206 \n",
            "Epochs: 1 / 10 Training Loss: 6.009 \n",
            "Epochs: 1 / 10 Training Loss: 5.294 \n",
            "Epochs: 1 / 10 Training Loss: 5.600 \n",
            "Epochs: 1 / 10 Training Loss: 6.078 \n",
            "Epochs: 1 / 10 Training Loss: 5.455 \n",
            "Epochs: 1 / 10 Training Loss: 5.538 \n",
            "Epochs: 1 / 10 Training Loss: 5.834 \n",
            "Epochs: 1 / 10 Training Loss: 6.441 \n",
            "Epochs: 1 / 10 Training Loss: 6.023 \n",
            "Epochs: 1 / 10 Training Loss: 6.794 \n",
            "Epochs: 1 / 10 Training Loss: 5.524 \n",
            "Epochs: 1 / 10 Training Loss: 6.045 \n",
            "Epochs: 1 / 10 Training Loss: 5.539 \n",
            "Epochs: 1 / 10 Training Loss: 5.811 \n",
            "Epochs: 1 / 10 Training Loss: 5.520 \n",
            "Epochs: 1 / 10 Training Loss: 5.526 \n",
            "Epochs: 1 / 10 Training Loss: 5.460 \n",
            "Epochs: 1 / 10 Training Loss: 5.924 \n",
            "Epochs: 1 / 10 Training Loss: 6.138 \n",
            "Epochs: 1 / 10 Training Loss: 6.265 \n",
            "Epochs: 1 / 10 Training Loss: 5.893 \n",
            "Epochs: 1 / 10 Training Loss: 6.228 \n",
            "Epochs: 1 / 10 Training Loss: 6.770 \n",
            "Epochs: 1 / 10 Training Loss: 5.933 \n",
            "Epochs: 1 / 10 Training Loss: 6.664 \n",
            "Epochs: 1 / 10 Training Loss: 6.274 \n",
            "Epochs: 1 / 10 Training Loss: 6.387 \n",
            "Epochs: 1 / 10 Training Loss: 6.225 \n",
            "Epochs: 1 / 10 Training Loss: 6.713 \n",
            "Epochs: 1 / 10 Training Loss: 6.297 \n",
            "Epochs: 1 / 10 Training Loss: 6.033 \n",
            "Epochs: 1 / 10 Training Loss: 5.450 \n",
            "Epochs: 1 / 10 Training Loss: 6.173 \n",
            "Epochs: 1 / 10 Training Loss: 5.539 \n",
            "Epochs: 1 / 10 Training Loss: 5.745 \n",
            "Epochs: 1 / 10 Training Loss: 5.566 \n",
            "Epochs: 1 / 10 Training Loss: 5.420 \n",
            "Epochs: 1 / 10 Training Loss: 5.828 \n",
            "Epochs: 1 / 10 Training Loss: 5.887 \n",
            "Epochs: 1 / 10 Training Loss: 6.390 \n",
            "Epochs: 1 / 10 Training Loss: 6.419 \n",
            "Epochs: 1 / 10 Training Loss: 6.335 \n",
            "Epochs: 1 / 10 Training Loss: 6.225 \n",
            "Epochs: 1 / 10 Training Loss: 5.832 \n",
            "Epochs: 1 / 10 Training Loss: 5.981 \n",
            "Epochs: 1 / 10 Training Loss: 6.368 \n",
            "Epochs: 1 / 10 Training Loss: 5.857 \n",
            "Epochs: 1 / 10 Training Loss: 5.065 \n",
            "Epochs: 1 / 10 Training Loss: 5.342 \n",
            "Epochs: 1 / 10 Training Loss: 5.809 \n",
            "Epochs: 1 / 10 Training Loss: 6.508 \n",
            "Epochs: 1 / 10 Training Loss: 5.744 \n",
            "Epochs: 1 / 10 Training Loss: 6.544 \n",
            "Epochs: 1 / 10 Training Loss: 6.040 \n",
            "Epochs: 1 / 10 Training Loss: 5.548 \n",
            "Epochs: 1 / 10 Training Loss: 6.224 \n",
            "Epochs: 1 / 10 Training Loss: 6.259 \n",
            "Epochs: 1 / 10 Training Loss: 6.113 \n",
            "Epochs: 1 / 10 Training Loss: 6.026 \n",
            "Epochs: 1 / 10 Training Loss: 5.807 \n",
            "Epochs: 1 / 10 Training Loss: 5.361 \n",
            "Epochs: 1 / 10 Training Loss: 5.065 \n",
            "Epochs: 1 / 10 Training Loss: 5.146 \n",
            "Epochs: 1 / 10 Training Loss: 5.904 \n",
            "Epochs: 1 / 10 Training Loss: 6.845 \n",
            "Epochs: 1 / 10 Training Loss: 6.616 \n",
            "Epochs: 1 / 10 Training Loss: 6.718 \n",
            "Epochs: 1 / 10 Training Loss: 6.344 \n",
            "Epochs: 1 / 10 Training Loss: 6.058 \n",
            "Epochs: 1 / 10 Training Loss: 6.095 \n",
            "Epochs: 1 / 10 Training Loss: 6.204 \n",
            "Epochs: 1 / 10 Training Loss: 5.390 \n",
            "Epochs: 1 / 10 Training Loss: 5.458 \n",
            "Epochs: 1 / 10 Training Loss: 5.575 \n",
            "Epochs: 1 / 10 Training Loss: 5.983 \n",
            "Epochs: 1 / 10 Training Loss: 5.306 \n",
            "Epochs: 1 / 10 Training Loss: 5.638 \n",
            "Epochs: 1 / 10 Training Loss: 5.739 \n",
            "Epochs: 1 / 10 Training Loss: 5.512 \n",
            "Epochs: 1 / 10 Training Loss: 5.776 \n",
            "Epochs: 1 / 10 Training Loss: 6.490 \n",
            "Epochs: 1 / 10 Training Loss: 6.293 \n",
            "Epochs: 1 / 10 Training Loss: 5.843 \n",
            "Epochs: 1 / 10 Training Loss: 6.329 \n",
            "Epochs: 1 / 10 Training Loss: 6.157 \n",
            "Epochs: 1 / 10 Training Loss: 6.326 \n",
            "Epochs: 1 / 10 Training Loss: 5.406 \n",
            "Epochs: 1 / 10 Training Loss: 5.650 \n",
            "Epochs: 1 / 10 Training Loss: 5.753 \n",
            "Epochs: 1 / 10 Training Loss: 6.071 \n",
            "Epochs: 1 / 10 Training Loss: 6.609 \n",
            "Epochs: 1 / 10 Training Loss: 6.539 \n",
            "Epochs: 1 / 10 Training Loss: 5.426 \n",
            "Epochs: 1 / 10 Training Loss: 5.633 \n",
            "Epochs: 1 / 10 Training Loss: 5.783 \n",
            "Epochs: 1 / 10 Training Loss: 5.868 \n",
            "Epochs: 1 / 10 Training Loss: 6.777 \n",
            "Epochs: 1 / 10 Training Loss: 5.338 \n",
            "Epochs: 1 / 10 Training Loss: 6.614 \n",
            "Epochs: 1 / 10 Training Loss: 6.324 \n",
            "Epochs: 1 / 10 Training Loss: 5.412 \n",
            "Epochs: 1 / 10 Training Loss: 6.383 \n",
            "Epochs: 1 / 10 Training Loss: 6.452 \n",
            "Epochs: 1 / 10 Training Loss: 6.085 \n",
            "Epochs: 1 / 10 Training Loss: 5.948 \n",
            "Epochs: 1 / 10 Training Loss: 5.867 \n",
            "Epochs: 1 / 10 Training Loss: 5.955 \n",
            "Epochs: 1 / 10 Training Loss: 5.793 \n",
            "Epochs: 1 / 10 Training Loss: 5.570 \n",
            "Epochs: 1 / 10 Training Loss: 5.898 \n",
            "Epochs: 1 / 10 Training Loss: 5.843 \n",
            "Epochs: 1 / 10 Training Loss: 5.246 \n",
            "Epochs: 1 / 10 Training Loss: 5.096 \n",
            "Epochs: 1 / 10 Training Loss: 5.924 \n",
            "Epochs: 1 / 10 Training Loss: 5.351 \n",
            "Epochs: 1 / 10 Training Loss: 5.940 \n",
            "Epochs: 1 / 10 Training Loss: 5.694 \n",
            "Epochs: 1 / 10 Training Loss: 6.496 \n",
            "Epochs: 1 / 10 Training Loss: 6.300 \n",
            "Epochs: 1 / 10 Training Loss: 6.563 \n",
            "Epochs: 1 / 10 Training Loss: 6.215 \n",
            "Epochs: 1 / 10 Training Loss: 6.241 \n",
            "Epochs: 1 / 10 Training Loss: 6.122 \n",
            "Epochs: 1 / 10 Training Loss: 5.898 \n",
            "Epochs: 1 / 10 Training Loss: 5.502 \n",
            "Epochs: 1 / 10 Training Loss: 6.464 \n",
            "Epochs: 1 / 10 Training Loss: 5.999 \n",
            "Epochs: 1 / 10 Training Loss: 5.763 \n",
            "Epochs: 1 / 10 Training Loss: 5.748 \n",
            "Epochs: 1 / 10 Training Loss: 6.778 \n",
            "Epochs: 1 / 10 Training Loss: 6.250 \n",
            "Epochs: 1 / 10 Training Loss: 6.040 \n",
            "Epochs: 1 / 10 Training Loss: 6.381 \n",
            "Epochs: 1 / 10 Training Loss: 5.198 \n",
            "Epochs: 1 / 10 Training Loss: 5.989 \n",
            "Epochs: 1 / 10 Training Loss: 5.692 \n",
            "Epochs: 1 / 10 Training Loss: 6.057 \n",
            "Epochs: 1 / 10 Training Loss: 5.978 \n",
            "Epochs: 1 / 10 Training Loss: 6.175 \n",
            "Epochs: 1 / 10 Training Loss: 6.109 \n",
            "Epochs: 1 / 10 Training Loss: 5.926 \n",
            "Epochs: 1 / 10 Training Loss: 5.805 \n",
            "Epochs: 1 / 10 Training Loss: 6.037 \n",
            "Epochs: 1 / 10 Training Loss: 5.763 \n",
            "Epochs: 1 / 10 Training Loss: 5.821 \n",
            "Epochs: 1 / 10 Training Loss: 5.522 \n",
            "Epochs: 1 / 10 Training Loss: 5.531 \n",
            "Epochs: 1 / 10 Training Loss: 5.439 \n",
            "Epochs: 1 / 10 Training Loss: 5.599 \n",
            "Epochs: 1 / 10 Training Loss: 5.783 \n",
            "Epochs: 1 / 10 Training Loss: 5.762 \n",
            "Epochs: 1 / 10 Training Loss: 6.292 \n",
            "Epochs: 1 / 10 Training Loss: 6.060 \n",
            "Epochs: 1 / 10 Training Loss: 6.522 \n",
            "Epochs: 1 / 10 Training Loss: 6.231 \n",
            "Epochs: 1 / 10 Training Loss: 5.651 \n",
            "Epochs: 1 / 10 Training Loss: 5.809 \n",
            "Epochs: 1 / 10 Training Loss: 5.459 \n",
            "Epochs: 1 / 10 Training Loss: 5.002 \n",
            "Epochs: 1 / 10 Training Loss: 5.255 \n",
            "Epochs: 1 / 10 Training Loss: 5.994 \n",
            "Epochs: 1 / 10 Training Loss: 5.760 \n",
            "Epochs: 1 / 10 Training Loss: 5.773 \n",
            "Epochs: 1 / 10 Training Loss: 5.516 \n",
            "Epochs: 1 / 10 Training Loss: 6.043 \n",
            "Epochs: 1 / 10 Training Loss: 6.227 \n",
            "Epochs: 1 / 10 Training Loss: 5.779 \n",
            "Epochs: 1 / 10 Training Loss: 6.333 \n",
            "Epochs: 1 / 10 Training Loss: 6.778 \n",
            "Epochs: 1 / 10 Training Loss: 5.775 \n",
            "Epochs: 1 / 10 Training Loss: 6.018 \n",
            "Epochs: 1 / 10 Training Loss: 6.487 \n",
            "Epochs: 1 / 10 Training Loss: 6.745 \n",
            "Epochs: 1 / 10 Training Loss: 5.374 \n",
            "Epochs: 1 / 10 Training Loss: 5.990 \n",
            "Epochs: 1 / 10 Training Loss: 6.015 \n",
            "Epochs: 1 / 10 Training Loss: 5.752 \n",
            "Epochs: 1 / 10 Training Loss: 5.739 \n",
            "Epochs: 1 / 10 Training Loss: 6.156 \n",
            "Epochs: 1 / 10 Training Loss: 5.218 \n",
            "Epochs: 1 / 10 Training Loss: 7.247 \n",
            "Epochs: 1 / 10 Training Loss: 6.548 \n",
            "Epochs: 1 / 10 Training Loss: 6.123 \n",
            "Epochs: 1 / 10 Training Loss: 5.685 \n",
            "Epochs: 1 / 10 Training Loss: 5.385 \n",
            "Epochs: 1 / 10 Training Loss: 5.655 \n",
            "Epochs: 1 / 10 Training Loss: 5.217 \n",
            "Epochs: 1 / 10 Training Loss: 5.473 \n",
            "Epochs: 1 / 10 Training Loss: 5.504 \n",
            "Epochs: 1 / 10 Training Loss: 5.732 \n",
            "Epochs: 1 / 10 Training Loss: 6.107 \n",
            "Epochs: 1 / 10 Training Loss: 5.737 \n",
            "Epochs: 1 / 10 Training Loss: 5.400 \n",
            "Epochs: 1 / 10 Training Loss: 5.329 \n",
            "Epochs: 1 / 10 Training Loss: 6.001 \n",
            "Epochs: 1 / 10 Training Loss: 5.746 \n",
            "Epochs: 1 / 10 Training Loss: 6.431 \n",
            "Epochs: 1 / 10 Training Loss: 5.475 \n",
            "Epochs: 1 / 10 Training Loss: 5.791 \n",
            "Epochs: 1 / 10 Training Loss: 5.676 \n",
            "Epochs: 1 / 10 Training Loss: 5.795 \n",
            "Epochs: 1 / 10 Training Loss: 5.846 \n",
            "Epochs: 1 / 10 Training Loss: 5.840 \n",
            "Epochs: 1 / 10 Training Loss: 5.053 \n",
            "Epochs: 1 / 10 Training Loss: 5.310 \n",
            "Epochs: 1 / 10 Training Loss: 5.536 \n",
            "Epochs: 1 / 10 Training Loss: 5.500 \n",
            "Epochs: 1 / 10 Training Loss: 6.105 \n",
            "Epochs: 1 / 10 Training Loss: 6.001 \n",
            "Epochs: 1 / 10 Training Loss: 5.862 \n",
            "Epochs: 1 / 10 Training Loss: 5.730 \n",
            "Epochs: 1 / 10 Training Loss: 6.122 \n",
            "Epochs: 1 / 10 Training Loss: 5.729 \n",
            "Epochs: 1 / 10 Training Loss: 4.854 \n",
            "Epochs: 1 / 10 Training Loss: 5.614 \n",
            "Epochs: 1 / 10 Training Loss: 5.849 \n",
            "Epochs: 1 / 10 Training Loss: 6.285 \n",
            "Epochs: 1 / 10 Training Loss: 6.421 \n",
            "Epochs: 1 / 10 Training Loss: 5.502 \n",
            "Epochs: 1 / 10 Training Loss: 6.008 \n",
            "Epochs: 1 / 10 Training Loss: 6.170 \n",
            "Epochs: 1 / 10 Training Loss: 6.493 \n",
            "Epochs: 1 / 10 Training Loss: 5.742 \n",
            "Epochs: 1 / 10 Training Loss: 6.040 \n",
            "Epochs: 1 / 10 Training Loss: 5.261 \n",
            "Epochs: 1 / 10 Training Loss: 6.240 \n",
            "Epochs: 1 / 10 Training Loss: 5.300 \n",
            "Epochs: 1 / 10 Training Loss: 5.998 \n",
            "Epochs: 1 / 10 Training Loss: 5.550 \n",
            "Epochs: 1 / 10 Training Loss: 5.533 \n",
            "Epochs: 1 / 10 Training Loss: 6.345 \n",
            "Epochs: 1 / 10 Training Loss: 6.134 \n",
            "Epochs: 1 / 10 Training Loss: 5.728 \n",
            "Epochs: 1 / 10 Training Loss: 6.334 \n",
            "Epochs: 1 / 10 Training Loss: 5.746 \n",
            "Epochs: 1 / 10 Training Loss: 5.840 \n",
            "Epochs: 1 / 10 Training Loss: 5.070 \n",
            "Epochs: 1 / 10 Training Loss: 5.737 \n",
            "Epochs: 1 / 10 Training Loss: 5.152 \n",
            "Epochs: 1 / 10 Training Loss: 6.143 \n",
            "Epochs: 1 / 10 Training Loss: 5.210 \n",
            "Epochs: 1 / 10 Training Loss: 5.176 \n",
            "Epochs: 1 / 10 Training Loss: 5.333 \n",
            "Epochs: 1 / 10 Training Loss: 5.323 \n",
            "Epochs: 1 / 10 Training Loss: 4.663 \n",
            "Epochs: 1 / 10 Training Loss: 5.610 \n",
            "Epochs: 1 / 10 Training Loss: 6.164 \n",
            "Epochs: 1 / 10 Training Loss: 6.142 \n",
            "Epochs: 1 / 10 Training Loss: 6.159 \n",
            "Epochs: 1 / 10 Training Loss: 5.799 \n",
            "Epochs: 1 / 10 Training Loss: 4.819 \n",
            "Epochs: 1 / 10 Training Loss: 5.659 \n",
            "Epochs: 1 / 10 Training Loss: 5.612 \n",
            "Epochs: 1 / 10 Training Loss: 5.720 \n",
            "Epochs: 1 / 10 Training Loss: 6.160 \n",
            "Epochs: 1 / 10 Training Loss: 6.460 \n",
            "Epochs: 1 / 10 Training Loss: 5.599 \n",
            "Epochs: 1 / 10 Training Loss: 6.052 \n",
            "Epochs: 1 / 10 Training Loss: 5.419 \n",
            "Epochs: 1 / 10 Training Loss: 5.942 \n",
            "Epochs: 1 / 10 Training Loss: 5.134 \n",
            "Epochs: 1 / 10 Training Loss: 5.704 \n",
            "Epochs: 1 / 10 Training Loss: 5.952 \n",
            "Epochs: 1 / 10 Training Loss: 5.684 \n",
            "Epochs: 1 / 10 Training Loss: 5.483 \n",
            "Epochs: 1 / 10 Training Loss: 5.532 \n",
            "Epochs: 1 / 10 Training Loss: 6.188 \n",
            "Epochs: 1 / 10 Training Loss: 5.467 \n",
            "Epochs: 1 / 10 Training Loss: 5.168 \n",
            "Epochs: 1 / 10 Training Loss: 5.709 \n",
            "Epochs: 1 / 10 Training Loss: 5.088 \n",
            "Epochs: 1 / 10 Training Loss: 5.526 \n",
            "Epochs: 1 / 10 Training Loss: 5.982 \n",
            "Epochs: 1 / 10 Training Loss: 5.537 \n",
            "Epochs: 1 / 10 Training Loss: 4.935 \n",
            "Epochs: 1 / 10 Training Loss: 5.758 \n",
            "Epochs: 1 / 10 Training Loss: 5.974 \n",
            "Epochs: 1 / 10 Training Loss: 5.620 \n",
            "Epochs: 1 / 10 Training Loss: 6.070 \n",
            "Epochs: 1 / 10 Training Loss: 5.881 \n",
            "Epochs: 1 / 10 Training Loss: 4.639 \n",
            "Epochs: 1 / 10 Training Loss: 5.787 \n",
            "Epochs: 1 / 10 Training Loss: 5.963 \n",
            "Epochs: 1 / 10 Training Loss: 6.191 \n",
            "Epochs: 1 / 10 Training Loss: 5.845 \n",
            "Epochs: 1 / 10 Training Loss: 5.547 \n",
            "Epochs: 1 / 10 Training Loss: 5.786 \n",
            "Epochs: 1 / 10 Training Loss: 5.982 \n",
            "Epochs: 1 / 10 Training Loss: 5.745 \n",
            "Epochs: 1 / 10 Training Loss: 6.200 \n",
            "Epochs: 1 / 10 Training Loss: 5.386 \n",
            "Epochs: 1 / 10 Training Loss: 6.523 \n",
            "Epochs: 1 / 10 Training Loss: 6.012 \n",
            "Epochs: 1 / 10 Training Loss: 6.042 \n",
            "Epochs: 1 / 10 Training Loss: 6.293 \n",
            "Epochs: 1 / 10 Training Loss: 6.079 \n",
            "Epochs: 1 / 10 Training Loss: 5.956 \n",
            "Epochs: 1 / 10 Training Loss: 6.046 \n",
            "Epochs: 1 / 10 Training Loss: 5.194 \n",
            "Epochs: 1 / 10 Training Loss: 5.676 \n",
            "Epochs: 1 / 10 Training Loss: 5.242 \n",
            "Epochs: 1 / 10 Training Loss: 6.007 \n",
            "Epochs: 1 / 10 Training Loss: 6.023 \n",
            "Epochs: 1 / 10 Training Loss: 5.502 \n",
            "Epochs: 1 / 10 Training Loss: 5.480 \n",
            "Epochs: 1 / 10 Training Loss: 5.647 \n",
            "Epochs: 1 / 10 Training Loss: 5.342 \n",
            "Epochs: 1 / 10 Training Loss: 5.775 \n",
            "Epochs: 1 / 10 Training Loss: 5.009 \n",
            "Epochs: 1 / 10 Training Loss: 5.112 \n",
            "Epochs: 1 / 10 Training Loss: 5.625 \n",
            "Epochs: 1 / 10 Training Loss: 5.411 \n",
            "Epochs: 1 / 10 Training Loss: 5.468 \n",
            "Epochs: 1 / 10 Training Loss: 5.155 \n",
            "Epochs: 1 / 10 Training Loss: 5.630 \n",
            "Epochs: 1 / 10 Training Loss: 5.552 \n",
            "Epochs: 1 / 10 Training Loss: 5.883 \n",
            "Epochs: 1 / 10 Training Loss: 5.381 \n",
            "Epochs: 1 / 10 Training Loss: 5.879 \n",
            "Epochs: 1 / 10 Training Loss: 5.885 \n",
            "Epochs: 1 / 10 Training Loss: 5.705 \n",
            "Epochs: 1 / 10 Training Loss: 5.424 \n",
            "Epochs: 1 / 10 Training Loss: 5.101 \n",
            "Epochs: 1 / 10 Training Loss: 6.243 \n",
            "Epochs: 1 / 10 Training Loss: 5.456 \n",
            "Epochs: 1 / 10 Training Loss: 5.565 \n",
            "Epochs: 1 / 10 Training Loss: 5.957 \n",
            "Epochs: 1 / 10 Training Loss: 6.498 \n",
            "Epochs: 1 / 10 Training Loss: 5.825 \n",
            "Epochs: 1 / 10 Training Loss: 5.751 \n",
            "Epochs: 1 / 10 Training Loss: 5.727 \n",
            "Epochs: 1 / 10 Training Loss: 5.535 \n",
            "Epochs: 1 / 10 Training Loss: 5.381 \n",
            "Epochs: 1 / 10 Training Loss: 5.590 \n",
            "Epochs: 1 / 10 Training Loss: 5.990 \n",
            "Epochs: 1 / 10 Training Loss: 5.579 \n",
            "Epochs: 1 / 10 Training Loss: 5.987 \n",
            "Epochs: 1 / 10 Training Loss: 6.144 \n",
            "Epochs: 1 / 10 Training Loss: 5.679 \n",
            "Epochs: 1 / 10 Training Loss: 5.330 \n",
            "Epochs: 1 / 10 Training Loss: 4.508 \n",
            "Epochs: 1 / 10 Training Loss: 5.385 \n",
            "Epochs: 1 / 10 Training Loss: 5.156 \n",
            "Epochs: 1 / 10 Training Loss: 5.825 \n",
            "Epochs: 1 / 10 Training Loss: 5.457 \n",
            "Epochs: 1 / 10 Training Loss: 5.328 \n",
            "Epochs: 1 / 10 Training Loss: 5.558 \n",
            "Epochs: 1 / 10 Training Loss: 5.299 \n",
            "Epochs: 1 / 10 Training Loss: 5.033 \n",
            "Epochs: 1 / 10 Training Loss: 5.966 \n",
            "Epochs: 1 / 10 Training Loss: 6.419 \n",
            "Epochs: 1 / 10 Training Loss: 5.583 \n",
            "Epochs: 1 / 10 Training Loss: 6.633 \n",
            "Epochs: 1 / 10 Training Loss: 5.935 \n",
            "Epochs: 1 / 10 Training Loss: 6.161 \n",
            "Epochs: 1 / 10 Training Loss: 6.406 \n",
            "Epochs: 1 / 10 Training Loss: 6.029 \n",
            "Epochs: 1 / 10 Training Loss: 5.371 \n",
            "Epochs: 1 / 10 Training Loss: 5.371 \n",
            "Epochs: 1 / 10 Training Loss: 6.364 \n",
            "Epochs: 1 / 10 Training Loss: 5.890 \n",
            "Epochs: 1 / 10 Training Loss: 5.870 \n",
            "Epochs: 1 / 10 Training Loss: 5.780 \n",
            "Epochs: 1 / 10 Training Loss: 6.026 \n",
            "Epochs: 1 / 10 Training Loss: 5.130 \n",
            "Epochs: 1 / 10 Training Loss: 5.752 \n",
            "Epochs: 1 / 10 Training Loss: 5.644 \n",
            "Epochs: 1 / 10 Training Loss: 5.385 \n",
            "Epochs: 1 / 10 Training Loss: 6.411 \n",
            "Epochs: 1 / 10 Training Loss: 6.599 \n",
            "Epochs: 1 / 10 Training Loss: 5.334 \n",
            "Epochs: 1 / 10 Training Loss: 6.042 \n",
            "Epochs: 1 / 10 Training Loss: 4.894 \n",
            "Epochs: 1 / 10 Training Loss: 5.674 \n",
            "Epochs: 1 / 10 Training Loss: 5.424 \n",
            "Epochs: 1 / 10 Training Loss: 5.088 \n",
            "Epochs: 1 / 10 Training Loss: 5.617 \n",
            "Epochs: 1 / 10 Training Loss: 5.208 \n",
            "Epochs: 1 / 10 Training Loss: 5.321 \n",
            "Epochs: 1 / 10 Training Loss: 5.029 \n",
            "Epochs: 1 / 10 Training Loss: 5.064 \n",
            "Epochs: 1 / 10 Training Loss: 5.335 \n",
            "Epochs: 1 / 10 Training Loss: 5.436 \n",
            "Epochs: 1 / 10 Training Loss: 5.624 \n",
            "Epochs: 1 / 10 Training Loss: 4.923 \n",
            "Epochs: 1 / 10 Training Loss: 4.903 \n",
            "Epochs: 1 / 10 Training Loss: 5.191 \n",
            "Epochs: 1 / 10 Training Loss: 4.836 \n",
            "Epochs: 1 / 10 Training Loss: 6.005 \n",
            "Epochs: 1 / 10 Training Loss: 5.816 \n",
            "Epochs: 1 / 10 Training Loss: 6.054 \n",
            "Epochs: 1 / 10 Training Loss: 6.560 \n",
            "Epochs: 1 / 10 Training Loss: 6.417 \n",
            "Epochs: 1 / 10 Training Loss: 5.764 \n",
            "Epochs: 1 / 10 Training Loss: 5.973 \n",
            "Epochs: 1 / 10 Training Loss: 6.319 \n",
            "Epochs: 1 / 10 Training Loss: 6.346 \n",
            "Epochs: 1 / 10 Training Loss: 5.902 \n",
            "Epochs: 1 / 10 Training Loss: 6.231 \n",
            "Epochs: 1 / 10 Training Loss: 5.901 \n",
            "Epochs: 1 / 10 Training Loss: 5.948 \n",
            "Epochs: 1 / 10 Training Loss: 6.250 \n",
            "Epochs: 1 / 10 Training Loss: 5.803 \n",
            "Epochs: 1 / 10 Training Loss: 5.693 \n",
            "Epochs: 1 / 10 Training Loss: 5.423 \n",
            "Epochs: 1 / 10 Training Loss: 6.055 \n",
            "Epochs: 1 / 10 Training Loss: 5.453 \n",
            "Epochs: 1 / 10 Training Loss: 5.038 \n",
            "Epochs: 1 / 10 Training Loss: 5.173 \n",
            "Epochs: 1 / 10 Training Loss: 5.372 \n",
            "Epochs: 1 / 10 Training Loss: 5.697 \n",
            "Epochs: 1 / 10 Training Loss: 6.036 \n",
            "Epochs: 1 / 10 Training Loss: 5.351 \n",
            "Epochs: 1 / 10 Training Loss: 6.454 \n",
            "Epochs: 1 / 10 Training Loss: 6.221 \n",
            "Epochs: 1 / 10 Training Loss: 5.920 \n",
            "Epochs: 1 / 10 Training Loss: 6.038 \n",
            "Epochs: 1 / 10 Training Loss: 5.898 \n",
            "Epochs: 1 / 10 Training Loss: 5.686 \n",
            "Epochs: 1 / 10 Training Loss: 5.365 \n",
            "Epochs: 1 / 10 Training Loss: 5.709 \n",
            "Epochs: 1 / 10 Training Loss: 5.948 \n",
            "Epochs: 1 / 10 Training Loss: 5.376 \n",
            "Epochs: 1 / 10 Training Loss: 5.468 \n",
            "Epochs: 1 / 10 Training Loss: 5.162 \n",
            "Epochs: 1 / 10 Training Loss: 5.860 \n",
            "Epochs: 1 / 10 Training Loss: 6.113 \n",
            "Epochs: 1 / 10 Training Loss: 6.092 \n",
            "Epochs: 1 / 10 Training Loss: 5.665 \n",
            "Epochs: 1 / 10 Training Loss: 5.827 \n",
            "Epochs: 1 / 10 Training Loss: 6.017 \n",
            "Epochs: 1 / 10 Training Loss: 5.633 \n",
            "Epochs: 1 / 10 Training Loss: 5.863 \n",
            "Epochs: 1 / 10 Training Loss: 6.080 \n",
            "Epochs: 1 / 10 Training Loss: 6.020 \n",
            "Epochs: 1 / 10 Training Loss: 5.965 \n",
            "Epochs: 1 / 10 Training Loss: 5.612 \n",
            "Epochs: 1 / 10 Training Loss: 5.706 \n",
            "Epochs: 1 / 10 Training Loss: 5.361 \n",
            "Epochs: 1 / 10 Training Loss: 5.494 \n",
            "Epochs: 1 / 10 Training Loss: 5.236 \n",
            "Epochs: 1 / 10 Training Loss: 5.417 \n",
            "Epochs: 1 / 10 Training Loss: 4.796 \n",
            "Epochs: 1 / 10 Training Loss: 5.509 \n",
            "Epochs: 1 / 10 Training Loss: 5.667 \n",
            "Epochs: 1 / 10 Training Loss: 5.916 \n",
            "Epochs: 1 / 10 Training Loss: 5.497 \n",
            "Epochs: 1 / 10 Training Loss: 5.607 \n",
            "Epochs: 1 / 10 Training Loss: 5.833 \n",
            "Epochs: 1 / 10 Training Loss: 5.708 \n",
            "Epochs: 1 / 10 Training Loss: 5.352 \n",
            "Epochs: 1 / 10 Training Loss: 4.780 \n",
            "Epochs: 1 / 10 Training Loss: 5.522 \n",
            "Epochs: 1 / 10 Training Loss: 4.716 \n",
            "Epochs: 1 / 10 Training Loss: 6.263 \n",
            "Epochs: 1 / 10 Training Loss: 4.813 \n",
            "Epochs: 1 / 10 Training Loss: 4.707 \n",
            "Epochs: 1 / 10 Training Loss: 4.423 \n",
            "Epochs: 1 / 10 Training Loss: 5.048 \n",
            "Epochs: 1 / 10 Training Loss: 5.091 \n",
            "Epochs: 1 / 10 Training Loss: 5.426 \n",
            "Epochs: 1 / 10 Training Loss: 4.269 \n",
            "Epochs: 1 / 10 Training Loss: 4.929 \n",
            "Epochs: 1 / 10 Training Loss: 4.125 \n",
            "Epochs: 1 / 10 Training Loss: 5.322 \n",
            "Epochs: 1 / 10 Training Loss: 6.045 \n",
            "Epochs: 1 / 10 Training Loss: 6.233 \n",
            "Epochs: 1 / 10 Training Loss: 5.568 \n",
            "Epochs: 1 / 10 Training Loss: 5.936 \n",
            "Epochs: 1 / 10 Training Loss: 6.101 \n",
            "Epochs: 1 / 10 Training Loss: 6.173 \n",
            "Epochs: 1 / 10 Training Loss: 6.000 \n",
            "Epochs: 1 / 10 Training Loss: 6.005 \n",
            "Epochs: 1 / 10 Training Loss: 6.403 \n",
            "Epochs: 1 / 10 Training Loss: 6.816 \n",
            "Epochs: 1 / 10 Training Loss: 6.566 \n",
            "Epochs: 1 / 10 Training Loss: 5.841 \n",
            "Epochs: 1 / 10 Training Loss: 6.644 \n",
            "Epochs: 1 / 10 Training Loss: 5.768 \n",
            "Epochs: 1 / 10 Training Loss: 5.035 \n",
            "Epochs: 1 / 10 Training Loss: 5.127 \n",
            "Epochs: 1 / 10 Training Loss: 5.029 \n",
            "Epochs: 1 / 10 Training Loss: 4.526 \n",
            "Epochs: 1 / 10 Training Loss: 4.555 \n",
            "Epochs: 1 / 10 Training Loss: 4.320 \n",
            "Epochs: 1 / 10 Training Loss: 3.963 \n",
            "Epochs: 1 / 10 Training Loss: 3.734 \n",
            "Epochs: 1 / 10 Training Loss: 3.780 \n",
            "Epochs: 1 / 10 Training Loss: 4.641 \n",
            "Epochs: 1 / 10 Training Loss: 6.446 \n",
            "Epochs: 1 / 10 Training Loss: 5.851 \n",
            "Epochs: 1 / 10 Training Loss: 5.302 \n",
            "Epochs: 1 / 10 Training Loss: 5.388 \n",
            "Epochs: 1 / 10 Training Loss: 6.362 \n",
            "Epochs: 1 / 10 Training Loss: 5.556 \n",
            "Epochs: 1 / 10 Training Loss: 6.084 \n",
            "Epochs: 1 / 10 Training Loss: 5.807 \n",
            "Epochs: 1 / 10 Training Loss: 6.856 \n",
            "Epochs: 1 / 10 Training Loss: 5.648 \n",
            "Epochs: 1 / 10 Training Loss: 5.767 \n",
            "Epochs: 1 / 10 Training Loss: 5.923 \n",
            "Epochs: 1 / 10 Training Loss: 6.057 \n",
            "Epochs: 1 / 10 Training Loss: 6.207 \n",
            "Epochs: 1 / 10 Training Loss: 5.595 \n",
            "Epochs: 1 / 10 Training Loss: 5.490 \n",
            "Epochs: 1 / 10 Training Loss: 5.727 \n",
            "Epochs: 1 / 10 Training Loss: 5.379 \n",
            "Epochs: 1 / 10 Training Loss: 5.024 \n",
            "Epochs: 1 / 10 Training Loss: 5.941 \n",
            "Epochs: 1 / 10 Training Loss: 5.300 \n",
            "Epochs: 1 / 10 Training Loss: 5.295 \n",
            "Epochs: 1 / 10 Training Loss: 5.298 \n",
            "Epochs: 1 / 10 Training Loss: 6.005 \n",
            "Epochs: 1 / 10 Training Loss: 5.650 \n",
            "Epochs: 1 / 10 Training Loss: 5.532 \n",
            "Epochs: 1 / 10 Training Loss: 5.675 \n",
            "Epochs: 1 / 10 Training Loss: 5.703 \n",
            "Epochs: 1 / 10 Training Loss: 5.690 \n",
            "Epochs: 1 / 10 Training Loss: 4.694 \n",
            "Epochs: 1 / 10 Training Loss: 5.778 \n",
            "Epochs: 1 / 10 Training Loss: 5.972 \n",
            "Epochs: 1 / 10 Training Loss: 5.796 \n",
            "Epochs: 1 / 10 Training Loss: 5.560 \n",
            "Epochs: 1 / 10 Training Loss: 6.063 \n",
            "Epochs: 1 / 10 Training Loss: 5.844 \n",
            "Epochs: 1 / 10 Training Loss: 5.391 \n",
            "Epochs: 1 / 10 Training Loss: 5.747 \n",
            "Epochs: 1 / 10 Training Loss: 5.514 \n",
            "Epochs: 1 / 10 Training Loss: 5.216 \n",
            "Epochs: 1 / 10 Training Loss: 6.052 \n",
            "Epochs: 1 / 10 Training Loss: 5.741 \n",
            "Epochs: 1 / 10 Training Loss: 5.906 \n",
            "Epochs: 1 / 10 Training Loss: 5.393 \n",
            "Epochs: 1 / 10 Training Loss: 5.468 \n",
            "Epochs: 1 / 10 Training Loss: 5.844 \n",
            "Epochs: 1 / 10 Training Loss: 6.526 \n",
            "Epochs: 1 / 10 Training Loss: 6.234 \n",
            "Epochs: 1 / 10 Training Loss: 5.617 \n",
            "Epochs: 1 / 10 Training Loss: 5.383 \n",
            "Epochs: 1 / 10 Training Loss: 4.915 \n",
            "Epochs: 1 / 10 Training Loss: 5.379 \n",
            "Epochs: 1 / 10 Training Loss: 5.775 \n",
            "Epochs: 1 / 10 Training Loss: 5.451 \n",
            "Epochs: 1 / 10 Training Loss: 5.964 \n",
            "Epochs: 1 / 10 Training Loss: 5.973 \n",
            "Epochs: 1 / 10 Training Loss: 6.158 \n",
            "Epochs: 1 / 10 Training Loss: 5.847 \n",
            "Epochs: 1 / 10 Training Loss: 5.594 \n",
            "Epochs: 1 / 10 Training Loss: 5.545 \n",
            "Epochs: 1 / 10 Training Loss: 5.494 \n",
            "Epochs: 1 / 10 Training Loss: 6.061 \n",
            "Epochs: 1 / 10 Training Loss: 5.869 \n",
            "Epochs: 1 / 10 Training Loss: 5.984 \n",
            "Epochs: 1 / 10 Training Loss: 5.436 \n",
            "Epochs: 1 / 10 Training Loss: 6.030 \n",
            "Epochs: 1 / 10 Training Loss: 5.249 \n",
            "Epochs: 1 / 10 Training Loss: 5.759 \n",
            "Epochs: 1 / 10 Training Loss: 5.997 \n",
            "Epochs: 1 / 10 Training Loss: 5.459 \n",
            "Epochs: 1 / 10 Training Loss: 5.799 \n",
            "Epochs: 1 / 10 Training Loss: 5.870 \n",
            "Epochs: 1 / 10 Training Loss: 5.563 \n",
            "Epochs: 1 / 10 Training Loss: 6.259 \n",
            "Epochs: 1 / 10 Training Loss: 5.457 \n",
            "Epochs: 1 / 10 Training Loss: 5.428 \n",
            "Epochs: 1 / 10 Training Loss: 5.772 \n",
            "Epochs: 1 / 10 Training Loss: 5.406 \n",
            "Epochs: 1 / 10 Training Loss: 6.133 \n",
            "Epochs: 1 / 10 Training Loss: 5.682 \n",
            "Epochs: 1 / 10 Training Loss: 5.347 \n",
            "Epochs: 1 / 10 Training Loss: 5.685 \n",
            "Epochs: 1 / 10 Training Loss: 6.057 \n",
            "Epochs: 1 / 10 Training Loss: 5.233 \n",
            "Epochs: 1 / 10 Training Loss: 6.517 \n",
            "Epochs: 1 / 10 Training Loss: 5.454 \n",
            "Epochs: 1 / 10 Training Loss: 5.582 \n",
            "Epochs: 1 / 10 Training Loss: 5.196 \n",
            "Epochs: 1 / 10 Training Loss: 6.026 \n",
            "Epochs: 1 / 10 Training Loss: 5.826 \n",
            "Epochs: 1 / 10 Training Loss: 5.805 \n",
            "Epochs: 1 / 10 Training Loss: 5.629 \n",
            "Epochs: 1 / 10 Training Loss: 5.906 \n",
            "Epochs: 1 / 10 Training Loss: 5.385 \n",
            "Epochs: 1 / 10 Training Loss: 5.571 \n",
            "Epochs: 1 / 10 Training Loss: 5.872 \n",
            "Epochs: 1 / 10 Training Loss: 5.001 \n",
            "Epochs: 1 / 10 Training Loss: 5.484 \n",
            "Epochs: 1 / 10 Training Loss: 5.286 \n",
            "Epochs: 1 / 10 Training Loss: 5.775 \n",
            "Epochs: 1 / 10 Training Loss: 5.961 \n",
            "Epochs: 1 / 10 Training Loss: 5.846 \n",
            "Epochs: 1 / 10 Training Loss: 5.591 \n",
            "Epochs: 1 / 10 Training Loss: 5.508 \n",
            "Epochs: 1 / 10 Training Loss: 6.212 \n",
            "Epochs: 1 / 10 Training Loss: 5.450 \n",
            "Epochs: 1 / 10 Training Loss: 5.044 \n",
            "Epochs: 1 / 10 Training Loss: 5.476 \n",
            "Epochs: 1 / 10 Training Loss: 5.942 \n",
            "Epochs: 1 / 10 Training Loss: 5.494 \n",
            "Epochs: 1 / 10 Training Loss: 6.100 \n",
            "Epochs: 1 / 10 Training Loss: 5.305 \n",
            "Epochs: 1 / 10 Training Loss: 5.881 \n",
            "Epochs: 1 / 10 Training Loss: 6.090 \n",
            "Epochs: 1 / 10 Training Loss: 5.636 \n",
            "Epochs: 1 / 10 Training Loss: 5.369 \n",
            "Epochs: 1 / 10 Training Loss: 5.900 \n",
            "Epochs: 1 / 10 Training Loss: 5.908 \n",
            "Epochs: 1 / 10 Training Loss: 5.793 \n",
            "Epochs: 1 / 10 Training Loss: 5.071 \n",
            "Epochs: 1 / 10 Training Loss: 5.323 \n",
            "Epochs: 1 / 10 Training Loss: 5.504 \n",
            "Epochs: 1 / 10 Training Loss: 4.885 \n",
            "Epochs: 1 / 10 Training Loss: 5.757 \n",
            "Epochs: 1 / 10 Training Loss: 5.584 \n",
            "Epochs: 1 / 10 Training Loss: 5.476 \n",
            "Epochs: 1 / 10 Training Loss: 5.582 \n",
            "Epochs: 1 / 10 Training Loss: 4.939 \n",
            "Epochs: 1 / 10 Training Loss: 5.159 \n",
            "Epochs: 1 / 10 Training Loss: 6.198 \n",
            "Epochs: 1 / 10 Training Loss: 5.543 \n",
            "Epochs: 1 / 10 Training Loss: 5.676 \n",
            "Epochs: 1 / 10 Training Loss: 5.703 \n",
            "Epochs: 1 / 10 Training Loss: 5.338 \n",
            "Epochs: 1 / 10 Training Loss: 5.184 \n",
            "Epochs: 1 / 10 Training Loss: 5.277 \n",
            "Epochs: 1 / 10 Training Loss: 5.929 \n",
            "Epochs: 1 / 10 Training Loss: 5.772 \n",
            "Epochs: 1 / 10 Training Loss: 5.328 \n",
            "Epochs: 1 / 10 Training Loss: 5.944 \n",
            "Epochs: 1 / 10 Training Loss: 5.064 \n",
            "Epochs: 1 / 10 Training Loss: 4.835 \n",
            "Epochs: 1 / 10 Training Loss: 5.614 \n",
            "Epochs: 1 / 10 Training Loss: 5.442 \n",
            "Epochs: 1 / 10 Training Loss: 5.638 \n",
            "Epochs: 1 / 10 Training Loss: 4.738 \n",
            "Epochs: 1 / 10 Training Loss: 4.649 \n",
            "Epochs: 1 / 10 Training Loss: 5.014 \n",
            "Epochs: 1 / 10 Training Loss: 5.394 \n",
            "Epochs: 1 / 10 Training Loss: 5.669 \n",
            "Epochs: 1 / 10 Training Loss: 5.700 \n",
            "Epochs: 1 / 10 Training Loss: 5.109 \n",
            "Epochs: 1 / 10 Training Loss: 5.779 \n",
            "Epochs: 1 / 10 Training Loss: 5.713 \n",
            "Epochs: 1 / 10 Training Loss: 5.374 \n",
            "Epochs: 1 / 10 Training Loss: 5.909 \n",
            "Epochs: 1 / 10 Training Loss: 5.000 \n",
            "Epochs: 1 / 10 Training Loss: 5.359 \n",
            "Epochs: 1 / 10 Training Loss: 5.237 \n",
            "Epochs: 1 / 10 Training Loss: 5.492 \n",
            "Epochs: 1 / 10 Training Loss: 5.374 \n",
            "Epochs: 1 / 10 Training Loss: 6.258 \n",
            "Epochs: 1 / 10 Training Loss: 6.103 \n",
            "Epochs: 1 / 10 Training Loss: 5.240 \n",
            "Epochs: 1 / 10 Training Loss: 5.887 \n",
            "Epochs: 1 / 10 Training Loss: 5.456 \n",
            "Epochs: 1 / 10 Training Loss: 6.150 \n",
            "Epochs: 1 / 10 Training Loss: 5.165 \n",
            "Epochs: 1 / 10 Training Loss: 5.442 \n",
            "Epochs: 1 / 10 Training Loss: 4.981 \n",
            "Epochs: 1 / 10 Training Loss: 5.748 \n",
            "Epochs: 1 / 10 Training Loss: 6.059 \n",
            "Epochs: 1 / 10 Training Loss: 5.606 \n",
            "Epochs: 1 / 10 Training Loss: 5.720 \n",
            "Epochs: 1 / 10 Training Loss: 6.551 \n",
            "Epochs: 1 / 10 Training Loss: 5.548 \n",
            "Epochs: 1 / 10 Training Loss: 5.361 \n",
            "Epochs: 1 / 10 Training Loss: 5.276 \n",
            "Epochs: 1 / 10 Training Loss: 5.133 \n",
            "Epochs: 1 / 10 Training Loss: 5.183 \n",
            "Epochs: 1 / 10 Training Loss: 5.458 \n",
            "Epochs: 1 / 10 Training Loss: 5.122 \n",
            "Epochs: 1 / 10 Training Loss: 6.338 \n",
            "Epochs: 1 / 10 Training Loss: 4.999 \n",
            "Epochs: 1 / 10 Training Loss: 5.717 \n",
            "Epochs: 1 / 10 Training Loss: 5.923 \n",
            "Epochs: 1 / 10 Training Loss: 5.557 \n",
            "Epochs: 1 / 10 Training Loss: 6.061 \n",
            "Epochs: 1 / 10 Training Loss: 6.591 \n",
            "Epochs: 1 / 10 Training Loss: 5.723 \n",
            "Epochs: 1 / 10 Training Loss: 5.588 \n",
            "Epochs: 1 / 10 Training Loss: 5.404 \n",
            "Epochs: 1 / 10 Training Loss: 5.454 \n",
            "Epochs: 1 / 10 Training Loss: 5.125 \n",
            "Epochs: 1 / 10 Training Loss: 5.890 \n",
            "Epochs: 1 / 10 Training Loss: 5.786 \n",
            "Epochs: 1 / 10 Training Loss: 5.556 \n",
            "Epochs: 1 / 10 Training Loss: 5.582 \n",
            "Epochs: 1 / 10 Training Loss: 5.319 \n",
            "Epochs: 1 / 10 Training Loss: 5.589 \n",
            "Epochs: 1 / 10 Training Loss: 5.105 \n",
            "Epochs: 1 / 10 Training Loss: 5.229 \n",
            "Epochs: 1 / 10 Training Loss: 4.856 \n",
            "Epochs: 1 / 10 Training Loss: 5.538 \n",
            "Epochs: 1 / 10 Training Loss: 5.784 \n",
            "Epochs: 1 / 10 Training Loss: 5.598 \n",
            "Epochs: 1 / 10 Training Loss: 6.325 \n",
            "Epochs: 1 / 10 Training Loss: 6.010 \n",
            "Epochs: 1 / 10 Training Loss: 6.084 \n",
            "Epochs: 1 / 10 Training Loss: 5.990 \n",
            "Epochs: 1 / 10 Training Loss: 6.067 \n",
            "Epochs: 1 / 10 Training Loss: 5.832 \n",
            "Epochs: 1 / 10 Training Loss: 5.706 \n",
            "Epochs: 1 / 10 Training Loss: 5.862 \n",
            "Epochs: 1 / 10 Training Loss: 5.666 \n",
            "Epochs: 1 / 10 Training Loss: 5.182 \n",
            "Epochs: 1 / 10 Training Loss: 5.450 \n",
            "Epochs: 1 / 10 Training Loss: 5.251 \n",
            "Epochs: 1 / 10 Training Loss: 5.376 \n",
            "Epochs: 1 / 10 Training Loss: 5.293 \n",
            "Epochs: 1 / 10 Training Loss: 4.991 \n",
            "Epochs: 1 / 10 Training Loss: 4.771 \n",
            "Epochs: 1 / 10 Training Loss: 5.326 \n",
            "Epochs: 1 / 10 Training Loss: 5.225 \n",
            "Epochs: 1 / 10 Training Loss: 5.118 \n",
            "Epochs: 1 / 10 Training Loss: 5.439 \n",
            "Epochs: 1 / 10 Training Loss: 5.286 \n",
            "Epochs: 1 / 10 Training Loss: 5.862 \n",
            "Epochs: 1 / 10 Training Loss: 5.423 \n",
            "Epochs: 1 / 10 Training Loss: 5.301 \n",
            "Epochs: 1 / 10 Training Loss: 5.352 \n",
            "Epochs: 1 / 10 Training Loss: 5.255 \n",
            "Epochs: 1 / 10 Training Loss: 5.055 \n",
            "Epochs: 1 / 10 Training Loss: 4.679 \n",
            "Epochs: 1 / 10 Training Loss: 4.952 \n",
            "Epochs: 1 / 10 Training Loss: 5.438 \n",
            "Epochs: 1 / 10 Training Loss: 5.103 \n",
            "Epochs: 1 / 10 Training Loss: 5.664 \n",
            "Epochs: 1 / 10 Training Loss: 5.859 \n",
            "Epochs: 1 / 10 Training Loss: 5.882 \n",
            "Epochs: 1 / 10 Training Loss: 6.247 \n",
            "Epochs: 1 / 10 Training Loss: 5.802 \n",
            "Epochs: 1 / 10 Training Loss: 5.575 \n",
            "Epochs: 1 / 10 Training Loss: 5.916 \n",
            "Epochs: 1 / 10 Training Loss: 5.371 \n",
            "Epochs: 1 / 10 Training Loss: 5.264 \n",
            "Epochs: 1 / 10 Training Loss: 5.451 \n",
            "Epochs: 1 / 10 Training Loss: 5.631 \n",
            "Epochs: 1 / 10 Training Loss: 5.477 \n",
            "Epochs: 1 / 10 Training Loss: 5.719 \n",
            "Epochs: 1 / 10 Training Loss: 5.124 \n",
            "Epochs: 1 / 10 Training Loss: 5.872 \n",
            "Epochs: 1 / 10 Training Loss: 5.319 \n",
            "Epochs: 1 / 10 Training Loss: 5.099 \n",
            "Epochs: 1 / 10 Training Loss: 5.121 \n",
            "Epochs: 1 / 10 Training Loss: 4.833 \n",
            "Epochs: 1 / 10 Training Loss: 6.258 \n",
            "Epochs: 1 / 10 Training Loss: 6.492 \n",
            "Epochs: 1 / 10 Training Loss: 5.160 \n",
            "Epochs: 1 / 10 Training Loss: 5.676 \n",
            "Epochs: 1 / 10 Training Loss: 5.768 \n",
            "Epochs: 1 / 10 Training Loss: 5.726 \n",
            "Epochs: 1 / 10 Training Loss: 5.341 \n",
            "Epochs: 1 / 10 Training Loss: 5.747 \n",
            "Epochs: 1 / 10 Training Loss: 5.144 \n",
            "Epochs: 1 / 10 Training Loss: 4.983 \n",
            "Epochs: 1 / 10 Training Loss: 5.712 \n",
            "Epochs: 1 / 10 Training Loss: 5.703 \n",
            "Epochs: 1 / 10 Training Loss: 5.453 \n",
            "Epochs: 1 / 10 Training Loss: 5.119 \n",
            "Epochs: 1 / 10 Training Loss: 5.342 \n",
            "Epochs: 1 / 10 Training Loss: 5.261 \n",
            "Epochs: 1 / 10 Training Loss: 4.823 \n",
            "Epochs: 1 / 10 Training Loss: 4.854 \n",
            "Epochs: 1 / 10 Training Loss: 5.440 \n",
            "Epochs: 1 / 10 Training Loss: 5.524 \n",
            "Epochs: 1 / 10 Training Loss: 4.826 \n",
            "Epochs: 1 / 10 Training Loss: 5.127 \n",
            "Epochs: 1 / 10 Training Loss: 5.998 \n",
            "Epochs: 1 / 10 Training Loss: 6.395 \n",
            "Epochs: 1 / 10 Training Loss: 5.567 \n",
            "Epochs: 1 / 10 Training Loss: 6.005 \n",
            "Epochs: 1 / 10 Training Loss: 5.319 \n",
            "Epochs: 1 / 10 Training Loss: 5.583 \n",
            "Epochs: 1 / 10 Training Loss: 5.355 \n",
            "Epochs: 1 / 10 Training Loss: 5.071 \n",
            "Epochs: 1 / 10 Training Loss: 5.712 \n",
            "Epochs: 1 / 10 Training Loss: 5.514 \n",
            "Epochs: 1 / 10 Training Loss: 5.926 \n",
            "Epochs: 1 / 10 Training Loss: 5.435 \n",
            "Epochs: 1 / 10 Training Loss: 5.500 \n",
            "Epochs: 1 / 10 Training Loss: 5.437 \n",
            "Epochs: 1 / 10 Training Loss: 5.229 \n",
            "Epochs: 1 / 10 Training Loss: 5.836 \n",
            "Epochs: 1 / 10 Training Loss: 5.926 \n",
            "Epochs: 1 / 10 Training Loss: 5.726 \n",
            "Epochs: 1 / 10 Training Loss: 6.168 \n",
            "Epochs: 1 / 10 Training Loss: 5.746 \n",
            "Epochs: 1 / 10 Training Loss: 5.998 \n",
            "Epochs: 1 / 10 Training Loss: 4.742 \n",
            "Epochs: 1 / 10 Training Loss: 5.088 \n",
            "Epochs: 1 / 10 Training Loss: 5.114 \n",
            "Epochs: 1 / 10 Training Loss: 5.345 \n",
            "Epochs: 1 / 10 Training Loss: 5.931 \n",
            "Epochs: 1 / 10 Training Loss: 5.243 \n",
            "Epochs: 1 / 10 Training Loss: 5.078 \n",
            "Epochs: 1 / 10 Training Loss: 4.961 \n",
            "Epochs: 1 / 10 Training Loss: 5.385 \n",
            "Epochs: 1 / 10 Training Loss: 5.880 \n",
            "Epochs: 1 / 10 Training Loss: 5.194 \n",
            "Epochs: 1 / 10 Training Loss: 5.109 \n",
            "Epochs: 1 / 10 Training Loss: 5.423 \n",
            "Epochs: 1 / 10 Training Loss: 5.665 \n",
            "Epochs: 1 / 10 Training Loss: 5.348 \n",
            "Epochs: 1 / 10 Training Loss: 5.144 \n",
            "Epochs: 1 / 10 Training Loss: 5.761 \n",
            "Epochs: 1 / 10 Training Loss: 5.703 \n",
            "Epochs: 1 / 10 Training Loss: 5.846 \n",
            "Epochs: 1 / 10 Training Loss: 6.257 \n",
            "Epochs: 1 / 10 Training Loss: 5.178 \n",
            "Epochs: 1 / 10 Training Loss: 6.100 \n",
            "Epochs: 1 / 10 Training Loss: 5.303 \n",
            "Epochs: 1 / 10 Training Loss: 5.493 \n",
            "Epochs: 1 / 10 Training Loss: 5.343 \n",
            "Epochs: 1 / 10 Training Loss: 5.578 \n",
            "Epochs: 1 / 10 Training Loss: 4.740 \n",
            "Epochs: 1 / 10 Training Loss: 5.704 \n",
            "Epochs: 1 / 10 Training Loss: 5.193 \n",
            "Epochs: 1 / 10 Training Loss: 5.825 \n",
            "Epochs: 1 / 10 Training Loss: 5.498 \n",
            "Epochs: 1 / 10 Training Loss: 4.898 \n",
            "Epochs: 1 / 10 Training Loss: 5.896 \n",
            "Epochs: 1 / 10 Training Loss: 6.132 \n",
            "Epochs: 1 / 10 Training Loss: 5.827 \n",
            "Epochs: 1 / 10 Training Loss: 5.545 \n",
            "Epochs: 1 / 10 Training Loss: 5.398 \n",
            "Epochs: 1 / 10 Training Loss: 5.384 \n",
            "Epochs: 1 / 10 Training Loss: 5.395 \n",
            "Epochs: 1 / 10 Training Loss: 5.599 \n",
            "Epochs: 1 / 10 Training Loss: 5.070 \n",
            "Epochs: 1 / 10 Training Loss: 5.584 \n",
            "Epochs: 1 / 10 Training Loss: 5.061 \n",
            "Epochs: 1 / 10 Training Loss: 4.877 \n",
            "Epochs: 1 / 10 Training Loss: 4.456 \n",
            "Epochs: 1 / 10 Training Loss: 4.837 \n",
            "Epochs: 1 / 10 Training Loss: 4.765 \n",
            "Epochs: 1 / 10 Training Loss: 5.015 \n",
            "Epochs: 1 / 10 Training Loss: 4.924 \n",
            "Epochs: 1 / 10 Training Loss: 5.041 \n",
            "Epochs: 1 / 10 Training Loss: 5.473 \n",
            "Epochs: 1 / 10 Training Loss: 5.820 \n",
            "Epochs: 1 / 10 Training Loss: 6.391 \n",
            "Epochs: 1 / 10 Training Loss: 6.493 \n",
            "Epochs: 1 / 10 Training Loss: 5.600 \n",
            "Epochs: 1 / 10 Training Loss: 5.787 \n",
            "Epochs: 1 / 10 Training Loss: 5.783 \n",
            "Epochs: 1 / 10 Training Loss: 6.212 \n",
            "Epochs: 1 / 10 Training Loss: 5.573 \n",
            "Epochs: 1 / 10 Training Loss: 5.759 \n",
            "Epochs: 1 / 10 Training Loss: 5.529 \n",
            "Epochs: 1 / 10 Training Loss: 5.799 \n",
            "Epochs: 1 / 10 Training Loss: 6.042 \n",
            "Epochs: 1 / 10 Training Loss: 5.701 \n",
            "Epochs: 1 / 10 Training Loss: 5.758 \n",
            "Epochs: 1 / 10 Training Loss: 6.414 \n",
            "Epochs: 1 / 10 Training Loss: 5.379 \n",
            "Epochs: 1 / 10 Training Loss: 5.520 \n",
            "Epochs: 1 / 10 Training Loss: 5.890 \n",
            "Epochs: 1 / 10 Training Loss: 5.959 \n",
            "Epochs: 1 / 10 Training Loss: 5.922 \n",
            "Epochs: 1 / 10 Training Loss: 6.316 \n",
            "Epochs: 1 / 10 Training Loss: 6.028 \n",
            "Epochs: 1 / 10 Training Loss: 6.332 \n",
            "Epochs: 1 / 10 Training Loss: 6.521 \n",
            "Epochs: 1 / 10 Training Loss: 5.189 \n",
            "training took -25 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwNuUJFuoO7k"
      },
      "source": [
        "# **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUy3YfBxoRZW"
      },
      "source": [
        "checkpoint_url = '/content/drive/MyDrive/Story_Teller/checkpoint3.pth'\n",
        "\n",
        "\n",
        "checkpoint = {'model': model,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'word_to_index': word_to_index,\n",
        "              'index_to_word': {i: word for i, word in enumerate(vocab)},\n",
        "              'epochs': epochs,\n",
        "              'average_loss': average_loss,\n",
        "              'device': device,\n",
        "              'optimizer_state': optimizer.state_dict(),\n",
        "              'batch_size': batch_size}\n",
        "\n",
        "torch.save(checkpoint, checkpoint_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DyoYf0qT__G"
      },
      "source": [
        "# **Load Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UarVOe0WTYEq",
        "outputId": "d02602a4-34ad-40f5-eb13-690b1616a152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = checkpoint['model']\n",
        "    model.optimizer_state = checkpoint['optimizer_state']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.device = checkpoint['device']\n",
        "    model.word_to_index = checkpoint['word_to_index']\n",
        "    model.index_to_word = checkpoint['index_to_word']\n",
        "    model.average_loss = checkpoint['average_loss']\n",
        "    return model\n",
        "\n",
        "checkpoint_url = '/content/drive/MyDrive/Story_Teller/checkpoint3.pth'\n",
        "model = load_checkpoint(checkpoint_url)\n",
        "index_to_word = model.index_to_word\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StoryTeller(\n",
              "  (embeddings): Embedding(166288, 5)\n",
              "  (linear1): Linear(in_features=25, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
              "  (linear3): Linear(in_features=512, out_features=166288, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXnfIAhyNNn1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "1fd57a3d-4392-468c-be0d-7c90997d2d6b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "loss_plot = pd.DataFrame(model.average_loss)\n",
        "loss_plot.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJKcqqCUp6I"
      },
      "source": [
        "# **Predict Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ess7PHw-UswI"
      },
      "source": [
        "def predict(model, first_words ,story_len ,top_k):\n",
        "    '''\n",
        "    param model: trained model\n",
        "    param first_words: a string of 5 (n_feature) words to begin the story\n",
        "    param story_len: an integer symbolizing the number of words you'd like the story to have\n",
        "    param top_k: the number of top probabilities per word that the network will randomly select from\n",
        "    '''\n",
        "    feature = (first_words.lower()).split(\" \")\n",
        "    for i in feature:\n",
        "        story.append(i)\n",
        "    for i in range(story_len):\n",
        "        feature_idx = torch.tensor([word_to_index[word] for word in feature], dtype=torch.long)\n",
        "        feature_idx = feature_idx.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.double().forward(feature_idx)\n",
        "        ps = torch.exp(output)\n",
        "        topk_combined = ps.topk(top_k, sorted=True)\n",
        "        #top kk probabilities\n",
        "        topk_ps = topk_combined[0][0]\n",
        "        #top kk classes\n",
        "        topk_class = topk_combined[1][0]\n",
        "        topk_class = [index_to_word[int(i)] for i in topk_class]\n",
        "        next_word = random.choice(topk_class)\n",
        "        feature = feature[1:]\n",
        "        feature.append(next_word)\n",
        "        story.append(next_word)\n",
        "    return story"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg6KWKpeU0GB"
      },
      "source": [
        "# **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLdMEMOqU2rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183d32e4-c98d-4ed0-936a-db7c942a6486"
      },
      "source": [
        "import random\n",
        "first_words = input('Type the first {} words to start the story:\\nexample: A lovely day at the\\n'.format(batch_size))\n",
        "\n",
        "top_k = 3\n",
        "story_len = 50\n",
        "story = []\n",
        "device = 'cuda:0'\n",
        "\n",
        "#Predicting and Handling User-Input Errors\n",
        "try:\n",
        "    prediction = predict(model, first_words, story_len, top_k)\n",
        "except KeyError as error:\n",
        "    print('Oops, looks like you\\'ve selected a word that the network does not understand yet: ', error)\n",
        "    if story[0] != \"\":\n",
        "        story = story[len(first_words):]\n",
        "    first_words = input('please select a different word:\\nexample: A lovely day at the\\n')\n",
        "    prediction = predict(model, first_words, story_len, top_k)\n",
        "except KeyError and RuntimeError:\n",
        "    if story[0] != \"\":\n",
        "        story = story[len(first_words):]\n",
        "    first_words = input('Oops, looks like you\\'ve typed {} words instead of {}!\\n\\nType the first 5 words to start the story:\\nexample: A lovely day at the\\n'.format(len(first_words.split(\" \")), n_features))\n",
        "    prediction = predict(model, first_words, story_len, top_k)\n",
        "\n",
        "print('-----------------------------------------------------\\n The STORY \\n-----------------------------------------------------')\n",
        "print(\" \".join(story))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type the first 5 words to start the story:\n",
            "example: A lovely day at the\n",
            "Another lovely day started as\n",
            "-----------------------------------------------------\n",
            " The STORY \n",
            "-----------------------------------------------------\n",
            "another lovely day started as they could the two man the bear had i will not to his bear ‘ i can and i was a bear and was his children ’ s and the bear and cried the forest was he had not and he took and he took his dwarf and they came\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step:\n",
        " experiment with losses and optimizers and epochs, hidden dimentions and embedding dimentions"
      ],
      "metadata": {
        "id": "58Qnc6lgPPuj"
      }
    }
  ]
}