{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NastasiaMazur/StoryTeller/blob/main/storyTeller2noValidation_demoFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbs4k0MJifjA",
        "outputId": "94e3e390-6722-4f02-c2bf-fc4d2cce1d8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#read-PDF imports here\n",
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "#pre-processing imports here\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBKuc_dR-Fgt",
        "outputId": "c46146ec-60d7-4ce0-df30-6ba69b1d797b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhwcUudNChx8"
      },
      "source": [
        "#file locations on drive\n",
        "grimm_url = '/content/drive/MyDrive/Story_Teller/FairytalesByTheBrothersGrimm.txt'\n",
        "coraline_url = '/content/drive/MyDrive/Story_Teller/Coraline.pdf'\n",
        "alice_url = '/content/drive/MyDrive/Story_Teller/AlicesAdvanturesInWonderland.txt'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIx1pysjBnD"
      },
      "source": [
        "#load punctuation symbols\n",
        "punct = string.punctuation"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r7JQDi3iz14"
      },
      "source": [
        "# **Pre-processing Coraline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vand5igZEP5l",
        "outputId": "453d1311-22bb-406b-dad6-15b8b953f6b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#a function to pre process Coraline by Neil Gaiman\n",
        "\n",
        "def preprocess_coraline(book):\n",
        "  '''\n",
        "  param book: url od a PDF book file\n",
        "  '''\n",
        "  output = \"\"\n",
        "  data = open(book, 'rb')\n",
        "  data = PyPDF2.PdfReader(data)\n",
        "  npages = len(data.pages)\n",
        "  for i in range(npages):\n",
        "    page_i = data.pages[i].extract_text()\n",
        "    output += page_i\n",
        "  output = output[1227:]\n",
        "  output = output.lower()\n",
        "  for word in output:\n",
        "    for char in word:\n",
        "        if char in punct:\n",
        "            word = word.replace(char, \"\")\n",
        "  remove_punct = \"\".join([word for word in output if word not in punct])\n",
        "  processed = word_tokenize(remove_punct)\n",
        "  print('Coraline database includes {} tokens, and {} unique tokens after editing'.format(len(processed), len(set(processed))))\n",
        "  return processed\n",
        "\n",
        "coraline = preprocess_coraline(coraline_url)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coraline database includes 33352 tokens, and 3660 unique tokens after editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRDl2y87jh1w"
      },
      "source": [
        "## **Preprocessing Alice in Wonderland**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6xyaeRuiMnk",
        "outputId": "465cb1eb-e45f-4c24-ab5b-89b4aaf36a39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#a function to pre process Alice's Advantures in Wonderland by Lewis Carroll\n",
        "\n",
        "def load_alice(text_file, punct, not_a_word):\n",
        "    '''\n",
        "    param text_file: url to Project Gutenberg's text file for Alice's Advantures in Wonderland by Lewis Carroll\n",
        "    param punct: a string of punctuation characters we'd like to filter\n",
        "    param not_a_word: a list of words we'd like to filter\n",
        "    '''\n",
        "    book = open(text_file, 'r')\n",
        "    book = book.read()\n",
        "    book = book[715:145060]\n",
        "    book_edit = re.sub('[+]', '', book)\n",
        "    book_edit = re.sub(r'(CHAPTER \\w+.\\s)', '', book)\n",
        "    words = word_tokenize(book_edit.lower())\n",
        "\n",
        "    word_list = []\n",
        "\n",
        "    # filtering punctuation and non-words\n",
        "    for word in words:\n",
        "        for char in word:\n",
        "            if char in punct:\n",
        "                word = word.replace(char, \"\")\n",
        "        if word not in punct and word not in not_a_word:\n",
        "            word_list.append(word)\n",
        "\n",
        "    print('Alice database includes {} tokens, and {} unique tokens after editing'.format(len(word_list), len(set(word_list))))\n",
        "    return word_list\n",
        "\n",
        "alice = load_alice(alice_url, (punct.replace('-', \"\") + '’' + '‘'), ['s', '--', 'nt', 've', 'll', 'd'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice database includes 26612 tokens, and 2596 unique tokens after editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcUOknolEWm"
      },
      "source": [
        "# **Preprocessing Grimm**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTG-MTfsEgNu",
        "outputId": "4dee958c-c5e4-4258-d3c1-0d97d3b1ccbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def load_fairytales(text_file):\n",
        "    '''\n",
        "    param text_file: url to Project Gutenberg's text file for Fairytales by The Brothers Grimm\n",
        "    '''\n",
        "    book = open(text_file, encoding='cp1252')\n",
        "    book = book.read()\n",
        "    book = book[2376:519859]\n",
        "    book_edit = re.sub('[(+*)]', '', book)\n",
        "    words = word_tokenize(book_edit.lower())\n",
        "\n",
        "    # filtering punctuation inside tokens (example: didn't or wow!)\n",
        "    for word in words:\n",
        "        for char in word:\n",
        "            if char in punct:\n",
        "                word = word.replace(char, \"\")\n",
        "\n",
        "    # filtering punctuation as alone standing tokens(example: \\ or ,)\n",
        "    words = [word for word in words if word not in punct]\n",
        "\n",
        "    print('Fairytales database includes {} tokens, and {} unique tokens after editing'.format(len(words), len(set(words))))\n",
        "    return words\n",
        "\n",
        "brothers_grimm = load_fairytales(grimm_url)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairytales database includes 106324 tokens, and 5335 unique tokens after editing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM8boZfBhTaw"
      },
      "source": [
        "# **Combined database including all books**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROjnov9uWSbe",
        "outputId": "be251a06-c254-4d87-e1bc-87f441ad3e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = coraline + alice + brothers_grimm\n",
        "data[:10]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['beaten',\n",
              " '—g',\n",
              " 'k',\n",
              " 'chesterton',\n",
              " '1',\n",
              " 'coraline',\n",
              " 'discovered',\n",
              " 'the',\n",
              " 'door',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84kEdfzunnVY"
      },
      "source": [
        "# **Convert Data into Numeric Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH5YsCDlm6jq",
        "outputId": "084038ea-cb91-4aa8-a4ae-6e7a5b995315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = set(data)\n",
        "vocab_size = len(data)\n",
        "\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "data = [word_to_index[word] for word in data]    # list comprehension\n",
        "\n",
        "data [:10]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6892, 2690, 3772, 2496, 3802, 7389, 7973, 2727, 3603, 1833]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index['beaten']"
      ],
      "metadata": {
        "id": "AAeR3ZE6uiKp",
        "outputId": "92e8bc56-8ae4-4aca-afb5-a56d54b35918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6892"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_IEytwGn0jC"
      },
      "source": [
        "# **Batching Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP5qnVKWn5gW",
        "outputId": "b6ceeced-b4b3-466a-df47-04747fc45953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 5 # look into first 5 words in each batch\n",
        "\n",
        "train_data = [([data[i], data[i+1],data[i+2], data[i+3], data[i+4]], data[i+5]) for i in range(vocab_size - batch_size)] #features + target word\n",
        "\n",
        "train_data[:10]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[([6892, 2690, 3772, 2496, 3802], 7389),\n",
              " ([2690, 3772, 2496, 3802, 7389], 7973),\n",
              " ([3772, 2496, 3802, 7389, 7973], 2727),\n",
              " ([2496, 3802, 7389, 7973, 2727], 3603),\n",
              " ([3802, 7389, 7973, 2727, 3603], 1833),\n",
              " ([7389, 7973, 2727, 3603, 1833], 4439),\n",
              " ([7973, 2727, 3603, 1833, 4439], 2282),\n",
              " ([2727, 3603, 1833, 4439, 2282], 6611),\n",
              " ([3603, 1833, 4439, 2282, 6611], 582),\n",
              " ([1833, 4439, 2282, 6611, 582], 5460)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU2YCOBVn6RU"
      },
      "source": [
        "# **Defining the Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMpb7olaoASw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "embedding_dim = 5\n",
        "\n",
        "class StoryTeller(nn.Module):\n",
        "  def __init__ (self, vocab_size, embedding_dim, batch_size):\n",
        "    super(StoryTeller, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(batch_size * embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, 512)\n",
        "    self.linear3 = nn.Linear(512, vocab_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((1,-1))\n",
        "    out = F.relu(self.liniar1(embeds))\n",
        "    out = F.relu(self.liniar2(out))\n",
        "    out = self.linear3(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = StoryTeller(vocab_size, embedding_dim, batch_size)\n",
        "model"
      ],
      "metadata": {
        "id": "pAmGIK5wzNWZ",
        "outputId": "85814b49-e62f-4dd7-9d72-2fdbc82e94c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StoryTeller(\n",
              "  (embeddings): Embedding(166288, 5)\n",
              "  (linear1): Linear(in_features=25, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=512, bias=True)\n",
              "  (linear3): Linear(in_features=512, out_features=166288, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcBctDGoA_5"
      },
      "source": [
        "# **Defining Training Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj_jF78uoGqy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kc2ysQGoJZU"
      },
      "source": [
        "# **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUYN20PVoMvn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwNuUJFuoO7k"
      },
      "source": [
        "# **Save Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUy3YfBxoRZW"
      },
      "source": [
        "checkpoint_url = '/content/drive/My Drive/Lessons/storyTeller/checkpoint3.pth'\n",
        "\n",
        "checkpoint = {'model': model,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'word_to_index': word_to_index,\n",
        "              'index_to_word': {i: word for i, word in enumerate(vocab)},\n",
        "              'epochs': epochs,\n",
        "              'average_loss': average_loss,\n",
        "              'device': device,\n",
        "              'optimizer_state': optimizer.state_dict(),\n",
        "              'batch_size': batch_size}\n",
        "\n",
        "torch.save(checkpoint, checkpoint_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DyoYf0qT__G"
      },
      "source": [
        "# **Load Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UarVOe0WTYEq",
        "outputId": "b20c9f5f-cb75-4c12-ceb4-4460b29ddab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = checkpoint['model']\n",
        "    model.optimizer_state = checkpoint['optimizer_state']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.device = checkpoint['device']\n",
        "    model.word_to_index = checkpoint['word_to_idxx']\n",
        "    model.index_to_word = checkpoint['idx_to_word']\n",
        "    model.average_loss = checkpoint['average_loss']\n",
        "    return model\n",
        "\n",
        "checkpoint_url = '/content/drive/My Drive/Lessons/storyTeller/checkpoint5.pth'\n",
        "model = load_checkpoint(checkpoint_url)\n",
        "index_to_word = model.index_to_word\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bcb5f10bf957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcheckpoint_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Lessons/storyTeller/checkpoint5.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mindex_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-bcb5f10bf957>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'StoryTeller' on <module '__main__'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXnfIAhyNNn1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "loss_plot = pd.DataFrame(model.average_loss)\n",
        "loss_plot.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgJKcqqCUp6I"
      },
      "source": [
        "# **Predict Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ess7PHw-UswI"
      },
      "source": [
        "def predict(model, first_words ,story_len ,top_k):\n",
        "    '''\n",
        "    param model: trained model\n",
        "    param first_words: a string of 5 (n_feature) words to begin the story\n",
        "    param story_len: an integer symbolizing the number of words you'd like the story to have\n",
        "    param top_k: the number of top probabilities per word that the network will randomly select from\n",
        "    '''\n",
        "    feature = (first_words.lower()).split(\" \")\n",
        "    for i in feature:\n",
        "        story.append(i)\n",
        "    for i in range(story_len):\n",
        "        feature_idx = torch.tensor([word_to_index[word] for word in feature], dtype=torch.long)\n",
        "        feature_idx = feature_idx.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.double().forward(feature_idx)\n",
        "        ps = torch.exp(output)\n",
        "        topk_combined = ps.topk(top_k, sorted=True)\n",
        "        #top kk probabilities\n",
        "        topk_ps = topk_combined[0][0]\n",
        "        #top kk classes\n",
        "        topk_class = topk_combined[1][0]\n",
        "        topk_class = [index_to_word[int(i)] for i in topk_class]\n",
        "        next_word = random.choice(topk_class)\n",
        "        feature = feature[1:]\n",
        "        feature.append(next_word)\n",
        "        story.append(next_word)\n",
        "    return story"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg6KWKpeU0GB"
      },
      "source": [
        "# **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLdMEMOqU2rn"
      },
      "source": [
        "import random\n",
        "first_words = input('Type the first {} words to start the story:\\nexample: A lovely day at the\\n'.format(batch_size))\n",
        "\n",
        "top_k = 3\n",
        "story_len = 50\n",
        "story = []\n",
        "device = 'cuda:0'\n",
        "\n",
        "#Predicting and Handling User-Input Errors\n",
        "try:\n",
        "    prediction = predict(model, first_words, story_len, top_k)\n",
        "except KeyError as error:\n",
        "    print('Oops, looks like you\\'ve selected a word that the network does not understand yet: ', error)\n",
        "    if story[0] != \"\":\n",
        "        story = story[len(first_words):]\n",
        "    first_words = input('please select a different word:\\nexample: A lovely day at the\\n')\n",
        "    prediction = predict(model, first_words, story_len, top_k)\n",
        "except KeyError and RuntimeError:\n",
        "    if story[0] != \"\":\n",
        "        story = story[len(first_words):]\n",
        "    first_words = input('Oops, looks like you\\'ve typed {} words instead of {}!\\n\\nType the first 5 words to start the story:\\nexample: A lovely day at the\\n'.format(len(first_words.split(\" \")), n_features))\n",
        "    prediction = predict(model, first_words, story_len, top_k)\n",
        "\n",
        "print('-----------------------------------------------------\\n The STORY \\n-----------------------------------------------------')\n",
        "print(\" \".join(story))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKJrOv8dkRn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}